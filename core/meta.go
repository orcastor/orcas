package core

import (
	"database/sql"
	"fmt"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	_ "github.com/mattn/go-sqlite3"
	b "github.com/orca-zhang/borm"
)

type BucketInfo struct {
	ID           int64  `borm:"id" json:"i,omitempty"`             // Bucket ID
	Name         string `borm:"name" json:"n,omitempty"`           // Bucket name
	UID          int64  `borm:"uid" json:"u,omitempty"`            // Owner
	Type         int    `borm:"type" json:"t,omitempty"`           // Bucket type, 0: none, 1: normal ...
	Quota        int64  `borm:"quota" json:"q,omitempty"`          // Quota, negative means unlimited
	Used         int64  `borm:"used" json:"s,omitempty"`           // Logical usage, counts original size of all versions
	RealUsed     int64  `borm:"real_used" json:"ru,omitempty"`     // Actual physical usage, counts actual stored data size
	LogicalUsed  int64  `borm:"logical_used" json:"lu,omitempty"`  // Logical occupancy, counts logical size of all valid objects (not deleted, PID >= 0) considering deduplication but excluding deleted objects
	DedupSavings int64  `borm:"dedup_savings" json:"ds,omitempty"` // Instant upload space savings, counts deduplicated data savings (LogicalUsed - unique data block size)
	ChunkSize    int64  `borm:"chunk_size" json:"cs,omitempty"`    // Chunk size (bytes), 0 or unset uses default 4MB
	// SnapshotID int64 // Latest snapshot version ID
}

type UserInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`     // User ID
	Usr    string `borm:"usr" json:"u,omitempty"`    // Username
	Pwd    string `borm:"pwd" json:"p,omitempty"`    // Password, encrypted using PBKDF2-HMAC-SHA256
	Key    string `borm:"key" json:"k,omitempty"`    // Database key
	Role   uint32 `borm:"role" json:"r,omitempty"`   // User role: regular user / administrator
	Name   string `borm:"name" json:"n,omitempty"`   // Name
	Avatar string `borm:"avatar" json:"a,omitempty"` // Avatar
}

// Object types
const (
	OBJ_TYPE_MALFORMED = iota - 1
	OBJ_TYPE_NONE
	OBJ_TYPE_DIR
	OBJ_TYPE_FILE
	OBJ_TYPE_VERSION
	OBJ_TYPE_PREVIEW
)

type ObjectInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`    // Object ID (randomly generated by idgen)
	PID    int64  `borm:"pid" json:"p,omitempty"`   // Parent object ID
	MTime  int64  `borm:"mtime" json:"m,omitempty"` // Update time, second-level timestamp
	DataID int64  `borm:"did" json:"d,omitempty"`   // Data ID, if 0, no data (newly created file, DataID is object ID, serving as first version data)
	Type   int    `borm:"type" json:"t,omitempty"`  // Object type, -1: malformed, 0: none, 1: dir, 2: file, 3: version, 4: preview(thumb/m3u8/pdf)
	Name   string `borm:"name" json:"n,omitempty"`  // Object name
	Size   int64  `borm:"size" json:"s,omitempty"`  // Object size, directory size is child object count, file size is latest version byte count
	Extra  string `borm:"ext" json:"e,omitempty"`   // Object extended information
}

// Data status
const (
	DATA_NORMAL         = uint32(1 << iota) // Normal
	DATA_ENDEC_AES256                       // Whether AES encrypted
	DATA_ENDEC_SM4                          // Whether SM4 encrypted
	DATA_ENDEC_RESERVED                     // Whether reserved encryption
	DATA_CMPR_SNAPPY                        // Whether snappy compressed
	DATA_CMPR_ZSTD                          // Whether zstd compressed
	DATA_CMPR_GZIP                          // Whether gzip compressed
	DATA_CMPR_BR                            // Whether brotli compressed
	DATA_KIND_IMG                           // Image type
	DATA_KIND_VIDEO                         // Video type
	DATA_KIND_AUDIO                         // Audio type
	DATA_KIND_ARCHIVE                       // Archive type
	DATA_KIND_DOCS                          // Document type
	DATA_KIND_FONT                          // Font type
	DATA_KIND_APP                           // Application type
	DATA_KIND_RESERVED                      // Unknown type
	DATA_SPARSE                             // Sparse file: chunks may not exist, read as zeros

	DATA_MALFORMED  = 0 // Whether corrupted
	DATA_ENDEC_MASK = DATA_ENDEC_AES256 | DATA_ENDEC_SM4 | DATA_ENDEC_RESERVED
	DATA_CMPR_MASK  = DATA_CMPR_SNAPPY | DATA_CMPR_ZSTD | DATA_CMPR_GZIP | DATA_CMPR_BR
	DATA_KIND_MASK  = DATA_KIND_IMG | DATA_KIND_VIDEO | DATA_KIND_AUDIO | DATA_KIND_ARCHIVE | DATA_KIND_DOCS | DATA_KIND_FONT | DATA_KIND_APP | DATA_KIND_RESERVED
)

type DataInfo struct {
	ID        int64  `borm:"id" json:"i,omitempty"`      // Data ID (randomly generated by idgen)
	Size      int64  `borm:"size" json:"s,omitempty"`    // Data size (compressed/encrypted)
	OrigSize  int64  `borm:"o_size" json:"r,omitempty"`  // Original data size (before compression/encryption)
	HdrCRC32  uint32 `borm:"h_crc32" json:"h,omitempty"` // CRC32 checksum of first 100KB
	CRC32     uint32 `borm:"crc32" json:"c,omitempty"`   // CRC32 checksum of entire original data
	MD5       int64  `borm:"md5" json:"m,omitempty"`     // MD5 hash of entire original data
	Cksum     uint32 `borm:"cksum" json:"u,omitempty"`   // CRC32 checksum of final data (for consistency audit)
	Kind      uint32 `borm:"kind" json:"k,omitempty"`    // Data status: normal, corrupted, encrypted, compressed, type (for preview, etc.)
	PkgID     int64  `borm:"pkg_id" json:"p,omitempty"`  // Package data ID (also generated by idgen)
	PkgOffset uint32 `borm:"pkg_off" json:"g,omitempty"` // Offset position in package data
	// PkgID != 0 indicates packaged data
	// SnapshotID int64 // Snapshot version ID
}

const EmptyDataID = 4708888888888

// WritingVersionName is the special name for "writing version" that allows direct data block modification
// Versions with name="0" can be directly modified without creating new versions
const WritingVersionName = "0"

func EmptyDataInfo() *DataInfo {
	return &DataInfo{
		ID:   EmptyDataID,
		MD5:  -1081059644736014743,
		Kind: DATA_NORMAL,
	}
}

// IsSparseFile checks if DataInfo represents a sparse file
// Sparse files have DATA_SPARSE flag set, meaning chunks may not exist and should be read as zeros
func IsSparseFile(dataInfo *DataInfo) bool {
	if dataInfo == nil {
		return false
	}
	return dataInfo.Kind&DATA_SPARSE != 0
}

// MarkSparseFile marks a DataInfo as sparse file
func MarkSparseFile(dataInfo *DataInfo) {
	if dataInfo != nil {
		dataInfo.Kind |= DATA_SPARSE
	}
}

const (
	BKT_TBL = "bkt"
	USR_TBL = "usr"

	OBJ_TBL  = "obj"
	DATA_TBL = "data"
)

type UserMetadataAdapter interface {
	PutUsr(c Ctx, u *UserInfo) error
	GetUsr(c Ctx, ids []int64) ([]*UserInfo, error)
	GetUsr2(c Ctx, usr string) (*UserInfo, error)
	SetUsr(c Ctx, fields []string, u *UserInfo) error
	ListUsers(c Ctx) ([]*UserInfo, error)
	DeleteUser(c Ctx, userID int64) error
}

type BucketMetadataAdapter interface {
	PutBkt(c Ctx, o []*BucketInfo) error
	DeleteBkt(c Ctx, bktID int64) error
	GetBkt(c Ctx, ids []int64) ([]*BucketInfo, error)
	ListBkt(c Ctx, uid int64) ([]*BucketInfo, error)
	ListAllBuckets(c Ctx) ([]*BucketInfo, error) // Get all buckets (for scheduled tasks)
	// Update bucket quota and usage
	UpdateBktQuota(c Ctx, bktID int64, quota int64) error
	// Increase bucket's actual usage (when uploading data)
	IncBktRealUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's actual usage (when deleting data)
	DecBktRealUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical usage (when creating objects, including instant upload)
	IncBktUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical usage (when deleting objects)
	DecBktUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical occupancy (when creating objects, only count valid objects)
	IncBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical occupancy (when deleting objects)
	DecBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's deduplication savings (saved data size from instant upload)
	IncBktDedupSavings(c Ctx, bktID int64, size int64) error
	// Decrease bucket's deduplication savings (when deleting objects)
	DecBktDedupSavings(c Ctx, bktID int64, size int64) error
}

// DuplicateGroup represents a group of duplicate data with same checksums
type DuplicateGroup struct {
	Key     string  // Checksum key: "OrigSize:HdrCRC32:CRC32:MD5"
	DataIDs []int64 // List of DataIDs with same checksums
}

type DataMetadataAdapter interface {
	RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error)
	PutData(c Ctx, bktID int64, d []*DataInfo) error
	GetData(c Ctx, bktID, id int64) (*DataInfo, error)
	ListAllData(c Ctx, bktID int64, offset, limit int) ([]*DataInfo, int64, error) // offset: offset, limit: page size, returns data and total count
	// Find duplicate data: returns DataID groups with same checksums (OrigSize, HdrCRC32, CRC32, MD5 all same)
	FindDuplicateData(c Ctx, bktID int64, offset, limit int) ([]DuplicateGroup, int64, error) // Returns duplicate data groups and total count
	// Update object's DataID reference
	UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error
	// Delete data metadata
	DeleteData(c Ctx, bktID int64, dataIDs []int64) error
	// Find small file data that can be packaged (for defragmentation)
	FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) ([]*DataInfo, int64, error)
}

type ObjectMetadataAdapter interface {
	PutObj(c Ctx, bktID int64, o []*ObjectInfo) ([]int64, error)
	GetObj(c Ctx, bktID int64, ids []int64) ([]*ObjectInfo, error)
	SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error
	ListObj(c Ctx, bktID, pid int64, wd, delim, order string, count int) ([]*ObjectInfo, int64, string, error)
	CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error)               // Count DataID references
	DeleteObj(c Ctx, bktID int64, id int64) error                                             // Delete object (mark as deleted, flip PID to negative)
	ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error)   // List deleted objects (PID < 0)
	ListRecycleBin(c Ctx, bktID int64, opt ListOptions) ([]*ObjectInfo, int64, string, error) // List recycle bin (objects with PID < 0)
	// Query all objects of specified type (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all child objects under specified directory (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all objects that reference the specified DataID
	GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error)
	// Query all versions of a file (sorted by MTime descending, latest first)
	// excludeWriting: if true, exclude writing versions (name="0")
	ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error)
}

type MetadataAdapter interface {
	Close()

	BucketMetadataAdapter
	UserMetadataAdapter
	DataMetadataAdapter
	ObjectMetadataAdapter
}

func GetDB(c ...interface{}) (*sql.DB, error) {
	param := "?_journal=WAL&cache=shared&mode=rwc&nolock=1"
	dirPath := ORCAS_BASE
	if len(c) > 1 {
		dirPath = filepath.Join(ORCAS_DATA, fmt.Sprint(c[1]))
		if c, ok := c[0].(Ctx); ok {
			if key := getKey(c); key != "" {
				param += "&key=" + key
			}
		}
	}
	os.MkdirAll(dirPath, 0o766)
	return sql.Open("sqlite3", filepath.Join(dirPath, "meta.db")+param)
}

func InitDB() error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	db.Exec(`CREATE TABLE IF NOT EXISTS bkt (id BIGINT PRIMARY KEY NOT NULL,
		uid BIGINT NOT NULL,
		quota BIGINT NOT NULL,
		used BIGINT NOT NULL,
		real_used BIGINT NOT NULL DEFAULT 0,
		logical_used BIGINT NOT NULL DEFAULT 0,
		dedup_savings BIGINT NOT NULL DEFAULT 0,
		type TINYINT NOT NULL,
		name TEXT NOT NULL,
		chunk_size BIGINT NOT NULL DEFAULT 0
	)`)

	// 添加新列的迁移（如果列不存在，忽略错误）
	// SQLite会在列已存在时返回错误，但我们可以安全地忽略它
	_, _ = db.Exec(`ALTER TABLE bkt ADD COLUMN logical_used BIGINT NOT NULL DEFAULT 0`)
	_, _ = db.Exec(`ALTER TABLE bkt ADD COLUMN dedup_savings BIGINT NOT NULL DEFAULT 0`)

	// 迁移旧列名usage到used（如果存在旧列但不存在新列）
	_, _ = db.Exec(`ALTER TABLE bkt ADD COLUMN used BIGINT NOT NULL DEFAULT 0`)
	_, _ = db.Exec(`UPDATE bkt SET used = usage WHERE used = 0 AND usage != 0`)

	db.Exec(`CREATE TABLE usr (id BIGINT PRIMARY KEY NOT NULL,
		role TINYINT NOT NULL,
		usr TEXT NOT NULL,
		pwd TEXT NOT NULL,
		name TEXT NOT NULL,
		avatar TEXT NOT NULL,
		key TEXT NOT NULL
	)`)

	db.Exec(`CREATE INDEX ix_uid on bkt (uid)`)
	db.Exec(`CREATE UNIQUE INDEX uk_name on bkt (name)`)
	db.Exec(`CREATE UNIQUE INDEX uk_usr on usr (usr)`)

	// Create default admin user if no admin exists
	initDefaultAdmin(db)

	return nil
}

// initDefaultAdmin creates a default admin user if no admin user exists
func initDefaultAdmin(db *sql.DB) {
	// Check if any admin user exists
	var count int
	err := db.QueryRow(`SELECT COUNT(*) FROM usr WHERE role = ?`, ADMIN).Scan(&count)
	if err != nil {
		return // If query fails, skip creating default admin
	}

	// If no admin exists, create default admin user
	if count == 0 {
		// Default admin credentials: username "orcas", password "orcas"
		hashedPwd, err := HashPassword("orcas")
		if err != nil {
			return // If password hashing fails, skip creating default admin
		}

		// Generate a new ID for the admin user
		adminID := NewID()
		if adminID <= 0 {
			return // If ID generation fails, skip creating default admin
		}

		// Create default admin user
		_, err = db.Exec(`INSERT INTO usr (id, role, usr, pwd, name, avatar, key) VALUES (?, ?, ?, ?, ?, ?, ?)`,
			adminID, ADMIN, "orcas", hashedPwd, "Administrator", "", "")
		if err != nil {
			// If insert fails (e.g., user already exists), ignore the error
			return
		}
	}
}

func InitBucketDB(c Ctx, bktID int64) error {
	db, err := GetDB(c, bktID)
	if err != nil {
		return fmt.Errorf("%w: %v", ERR_OPEN_DB, err)
	}
	defer db.Close()

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS obj (id BIGINT PRIMARY KEY NOT NULL,
		pid BIGINT NOT NULL,
		did BIGINT NOT NULL,
		size BIGINT NOT NULL,
		mtime BIGINT NOT NULL,
		type TINYINT NOT NULL,
		name TEXT NOT NULL,
		ext TEXT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create obj table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS data (id BIGINT PRIMARY KEY NOT NULL,
		size BIGINT NOT NULL,
		o_size BIGINT NOT NULL,
		md5 BIGINT NOT NULL,
		pkg_id BIGINT NOT NULL,
		pkg_off INTEGER NOT NULL,
		h_crc32 INTEGER NOT NULL,
		crc32 INTEGER NOT NULL,
		cksum INTEGER NOT NULL,
		kind SMALLINT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create data table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_pid_name on obj (pid, name)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_pid_name: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_ref ON data (o_size, h_crc32, crc32, md5)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_ref: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`PRAGMA temp_store = MEMORY`)
	if err != nil {
		return fmt.Errorf("%w: set pragma: %v", ERR_EXEC_DB, err)
	}
	return nil
}

type DefaultMetadataAdapter struct{}

func (dma *DefaultMetadataAdapter) Close() {
}

func (dma *DefaultMetadataAdapter) RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	tbl := fmt.Sprintf("tmp_%x", time.Now().UnixNano())
	// 创建临时表
	db.Exec(`CREATE TEMPORARY TABLE ` + tbl + ` (o_size BIGINT NOT NULL,
		h_crc32 UNSIGNED INT NOT NULL,
		crc32 UNSIGNED INT NOT NULL,
		md5 BIGINT NOT NULL
	)`)
	// 把待查询数据放到临时表
	if _, err = b.TableContext(c, db, tbl).Insert(&d,
		b.Fields("o_size", "h_crc32", "crc32", "md5")); err != nil {
		return nil, ERR_EXEC_DB
	}
	var refs []struct {
		ID       int64  `borm:"max(a.id)"`
		OrigSize int64  `borm:"b.o_size"`
		HdrCRC32 uint32 `borm:"b.h_crc32"`
		CRC32    uint32 `borm:"b.crc32"`
		MD5      int64  `borm:"b.md5"`
	}
	// 联表查询
	if _, err = b.TableContext(c, db, `data a, `+tbl+` b`).Select(&refs,
		b.Join(`on a.o_size=b.o_size and a.h_crc32=b.h_crc32 and 
			(b.crc32=0 or b.md5=0 or (a.crc32=b.crc32 and a.md5=b.md5))`),
		b.GroupBy("b.o_size", "b.h_crc32", "b.crc32", "b.md5")); err != nil {
		return nil, ERR_QUERY_DB
	}
	// 删除临时表
	db.Exec(`DROP TABLE ` + tbl)

	// 构造辅助查询map
	aux := make(map[string]int64, 0)
	for _, ref := range refs {
		aux[fmt.Sprintf("%d:%d:%d:%d", ref.OrigSize, ref.HdrCRC32, ref.CRC32, ref.MD5)] = ref.ID
	}

	res := make([]int64, len(d))
	for i, x := range d {
		// 如果最基础的数据不完整，直接跳过
		if x.OrigSize == 0 || x.HdrCRC32 == 0 {
			continue
		}

		key := fmt.Sprintf("%d:%d:%d:%d", x.OrigSize, x.HdrCRC32, x.CRC32, x.MD5)
		if id, ok := aux[key]; ok {
			// Found in database or current batch
			if id < 0 {
				// Negative ID means reference to another element in current batch (using two's complement)
				// Return the negative index directly (^index format)
				res[i] = id
			} else {
				// Positive ID means found in database
				// 全文件的数据没有，说明是预Ref
				if x.CRC32 == 0 || x.MD5 == 0 {
					if id > 0 {
						res[i] = 1 // 非0代表预Ref成功，预Ref只看数据库
					}
				} else {
					res[i] = id
				}
			}
		} else {
			// 没有秒传成功，但是当前批次可能有一样的数据
			// Store negative index (^i) for future elements in this batch to reference
			aux[key] = int64(^i)
		}
	}
	return res, nil
}

func (dma *DefaultMetadataAdapter) PutData(c Ctx, bktID int64, d []*DataInfo) error {
	db, err := GetDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, DATA_TBL).ReplaceInto(&d); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetData(c Ctx, bktID, id int64) (d *DataInfo, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	d = &DataInfo{}
	if _, err = b.TableContext(c, db, DATA_TBL).Select(d, b.Where(b.Eq("id", id))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListAllData(c Ctx, bktID int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	defer db.Close()

	// Get total count
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&total, b.Fields("count(1)")); err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Paginated query: use borm to implement LIMIT and OFFSET
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		if _, err = b.TableContext(c, db, DATA_TBL).Select(&d); err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) FindDuplicateData(c Ctx, bktID int64, offset, limit int) (groups []DuplicateGroup, total int64, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	defer db.Close()

	// 查找具有相同校验值的重复数据（需要OrigSize, HdrCRC32, CRC32, MD5都相同）
	// 首先获取总数（重复组数）
	query := `SELECT COUNT(*) FROM (
		SELECT o_size, h_crc32, crc32, md5, COUNT(*) as cnt
		FROM data
		WHERE o_size > 0 AND h_crc32 > 0 AND crc32 > 0 AND md5 != 0
		GROUP BY o_size, h_crc32, crc32, md5
		HAVING cnt > 1
	)`
	err = db.QueryRow(query).Scan(&total)
	if err != nil {
		return nil, 0, ERR_QUERY_DB
	}
	if total == 0 {
		return groups, 0, nil
	}

	// 分页查询重复数据组
	if limit > 0 {
		query = fmt.Sprintf(`SELECT o_size, h_crc32, crc32, md5, GROUP_CONCAT(id) as ids
			FROM data
			WHERE o_size > 0 AND h_crc32 > 0 AND crc32 > 0 AND md5 != 0
			GROUP BY o_size, h_crc32, crc32, md5
			HAVING COUNT(*) > 1
			ORDER BY MIN(id)
			LIMIT %d OFFSET %d`, limit, offset)
	} else {
		query = `SELECT o_size, h_crc32, crc32, md5, GROUP_CONCAT(id) as ids
			FROM data
			WHERE o_size > 0 AND h_crc32 > 0 AND crc32 > 0 AND md5 != 0
			GROUP BY o_size, h_crc32, crc32, md5
			HAVING COUNT(*) > 1
			ORDER BY MIN(id)`
	}

	rows, err := db.Query(query)
	if err != nil {
		return nil, 0, ERR_QUERY_DB
	}
	defer rows.Close()

	for rows.Next() {
		var oSize int64
		var hCrc32, crc32 uint32
		var md5 int64
		var idsStr string
		if err = rows.Scan(&oSize, &hCrc32, &crc32, &md5, &idsStr); err != nil {
			continue
		}

		// 解析ID列表
		idStrs := strings.Split(idsStr, ",")
		dataIDs := make([]int64, 0, len(idStrs))
		for _, idStr := range idStrs {
			id, err2 := strconv.ParseInt(strings.TrimSpace(idStr), 10, 64)
			if err2 != nil {
				continue
			}
			dataIDs = append(dataIDs, id)
		}

		if len(dataIDs) > 1 {
			key := fmt.Sprintf("%d:%d:%d:%d", oSize, hCrc32, crc32, md5)
			groups = append(groups, DuplicateGroup{
				Key:     key,
				DataIDs: dataIDs,
			})
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error {
	db, err := GetDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	// 更新所有引用oldDataID的对象，将其DataID改为newDataID
	_, err = db.Exec("UPDATE obj SET did = ? WHERE did = ?", newDataID, oldDataID)
	if err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) DeleteData(c Ctx, bktID int64, dataIDs []int64) error {
	if len(dataIDs) == 0 {
		return nil
	}
	db, err := GetDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	// 批量删除数据元信息
	_, err = b.TableContext(c, db, DATA_TBL).Delete(b.Where(b.In("id", dataIDs)))
	if err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	defer db.Close()

	// Find small file data: pkg_id=0 (not packaged), o_size < maxSize
	// Get total count
	query := fmt.Sprintf("SELECT COUNT(*) FROM data WHERE pkg_id = 0 AND o_size > 0 AND o_size < %d", maxSize)
	err = db.QueryRow(query).Scan(&total)
	if err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Paginated query: use borm to implement LIMIT and OFFSET
	conds := []interface{}{
		b.Eq("pkg_id", 0),
		b.Gt("o_size", 0),
		b.Lt("o_size", maxSize),
	}
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.Where(conds...),
			b.OrderBy("id"))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) PutObj(c Ctx, bktID int64, o []*ObjectInfo) (ids []int64, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	for _, x := range o {
		ids = append(ids, x.ID)
	}

	var n int
	if n, err = b.TableContext(c, db, OBJ_TBL).InsertIgnore(&o); err != nil {
		return nil, ERR_EXEC_DB
	}
	if n != len(o) {
		var inserted []int64
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&inserted, b.Fields("id"), b.Where(b.In("id", ids))); err != nil {
			return nil, ERR_QUERY_DB
		}
		// 处理有冲突的情况
		m := make(map[int64]struct{}, 0)
		for _, v := range inserted {
			m[v] = struct{}{}
		}
		// 擦除没有插入成功的id
		for i, id := range ids {
			if _, ok := m[id]; !ok {
				ids[i] = 0
			}
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) GetObj(c Ctx, bktID int64, ids []int64) (o []*ObjectInfo, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error {
	db, err := GetDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(o, b.Fields(fields...), b.Where(b.Eq("id", o.ID))); err != nil {
		// 如果存在同名文件，会报错：Error: stepping, UNIQUE constraint failed: obj.name (19)
		if strings.Contains(err.Error(), "UNIQUE constraint failed") {
			return ERR_DUP_KEY
		}
		return ERR_EXEC_DB
	}
	return nil
}

func toDelim(field string, o *ObjectInfo) string {
	var d interface{}
	switch field {
	case "id":
		return fmt.Sprint(o.ID)
	case "name":
		return fmt.Sprint(o.Name)
	case "mtime":
		d = o.MTime
	case "size":
		d = o.Size
	case "type":
		d = o.Type
	}
	return fmt.Sprintf("%v:%d", d, o.ID)
}

func doOrder(delim, order string, conds *[]interface{}) (string, string) {
	// 处理order
	if order == "" {
		order = "id"
	}
	fn := b.Gt
	orderBy := order
	switch order[0] {
	case '-':
		fn = b.Lt
		order = order[1:]
		orderBy = order + " desc"
	case '+':
		order = order[1:]
		orderBy = order
	}
	if order != "id" && order != "name" {
		orderBy = orderBy + ", id"
	}

	// 处理边界条件
	ds := strings.Split(delim, ":")
	if len(ds) > 0 && ds[0] != "" {
		if order == "id" || order == "name" {
			*conds = append(*conds, fn(order, ds[0]))
		} else if len(ds) == 2 {
			*conds = append(*conds, b.Or(fn(order, ds[0]),
				b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
		}
	}
	return orderBy, order
}

func (dma *DefaultMetadataAdapter) ListObj(c Ctx, bktID, pid int64,
	wd, delim, order string, count int) (o []*ObjectInfo,
	cnt int64, d string, err error,
) {
	conds := []interface{}{b.Eq("pid", pid)}
	if wd != "" {
		if strings.ContainsAny(wd, "*?") {
			// sqlite 分支使用 LIKE 模式匹配
			conds = append(conds, fmt.Sprintf("name LIKE '%s'", strings.ReplaceAll(strings.ReplaceAll(wd, "*", "%"), "?", "_")))
		} else {
			conds = append(conds, b.Eq("name", wd))
		}
	}

	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, "", ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", ERR_QUERY_DB
	}

	if count > 0 {
		var orderBy string
		orderBy, order = doOrder(delim, order, &conds)
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(count)); err != nil {
			return nil, 0, "", ERR_QUERY_DB
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) PutUsr(c Ctx, u *UserInfo) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, USR_TBL).ReplaceInto(&u); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetUsr(c Ctx, ids []int64) (o []*UserInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, USR_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) GetUsr2(c Ctx, usr string) (o *UserInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	o = &UserInfo{}
	if _, err = b.TableContext(c, db, USR_TBL).Select(o, b.Where(b.Eq("usr", usr))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) SetUsr(c Ctx, fields []string, u *UserInfo) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, USR_TBL).Update(&u,
		b.Fields(fields...), b.Where(b.Eq("id", u.ID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) ListUsers(c Ctx) (o []*UserInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	// Use raw SQL query directly to avoid borm issues
	query := "SELECT id, role, usr, pwd, name, avatar, key FROM " + USR_TBL
	rows, err := db.Query(query)
	if err != nil {
		return nil, ERR_QUERY_DB
	}
	defer rows.Close()

	o = []*UserInfo{}
	for rows.Next() {
		u := &UserInfo{}
		if err = rows.Scan(&u.ID, &u.Role, &u.Usr, &u.Pwd, &u.Name, &u.Avatar, &u.Key); err != nil {
			continue
		}
		o = append(o, u)
	}
	if err != nil && err != rows.Err() {
		return nil, ERR_QUERY_DB
	}
	// 清除敏感信息
	for i := range o {
		o[i].Pwd = ""
		o[i].Key = ""
	}
	return
}

func (dma *DefaultMetadataAdapter) DeleteUser(c Ctx, userID int64) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, USR_TBL).Delete(b.Where(b.Eq("id", userID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) PutBkt(c Ctx, o []*BucketInfo) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).ReplaceInto(&o); err != nil {
		return ERR_EXEC_DB
	}
	for _, x := range o {
		InitBucketDB(c, x.ID)
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetBkt(c Ctx, ids []int64) (o []*BucketInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListBkt(c Ctx, uid int64) (o []*BucketInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o, b.Where(b.Eq("uid", uid))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListAllBuckets(c Ctx) (o []*BucketInfo, err error) {
	db, err := GetDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) DeleteBkt(c Ctx, bktID int64) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).Delete(b.Where(b.Eq("id", bktID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) UpdateBktQuota(c Ctx, bktID int64, quota int64) error {
	db, err := GetDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	if _, err = b.TableContext(c, db, BKT_TBL).Update(&BucketInfo{Quota: quota},
		b.Fields("quota"), b.Where(b.Eq("id", bktID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, size, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, -size, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, size, 0, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, -size, 0, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, size, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, -size, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, size)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, -size)
	return nil
}

func (dma *DefaultMetadataAdapter) CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	if len(dataIDs) == 0 {
		return make(map[int64]int64), nil
	}

	// 统计每个DataID被引用的次数
	type refCount struct {
		DataID int64 `borm:"did"`
		Count  int64 `borm:"count(1)"`
	}
	var counts []refCount
	// 排除已删除的对象（pid >= 0，因为删除时PID会被翻转成负数）
	whereConds := []interface{}{b.In("did", dataIDs), "pid >= 0"}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&counts,
		b.Fields("did", "count(1)"),
		b.Where(whereConds...),
		b.GroupBy("did")); err != nil {
		return nil, ERR_QUERY_DB
	}

	result := make(map[int64]int64)
	for _, c := range counts {
		result[c.DataID] = c.Count
	}
	// 确保所有查询的dataID都在结果中（即使引用数为0）
	for _, dataID := range dataIDs {
		if _, exists := result[dataID]; !exists {
			result[dataID] = 0
		}
	}
	return result, nil
}

func (dma *DefaultMetadataAdapter) ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	defer db.Close()

	// Get total count
	var cnt int64
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(b.Eq("type", objType), "pid >= 0"))
	if err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var objs []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(b.Eq("type", objType), "pid >= 0"),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return objs, cnt, nil
}

func (dma *DefaultMetadataAdapter) ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	defer db.Close()

	// Get total count
	var cnt int64
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(b.Eq("pid", pid), "pid >= 0"))
	if err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var children []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&children,
			b.Where(b.Eq("pid", pid), "pid >= 0"),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return children, cnt, nil
}

func (dma *DefaultMetadataAdapter) GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	var objs []*ObjectInfo
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
		b.Where(b.Eq("did", dataID)))
	if err != nil {
		return nil, ERR_QUERY_DB
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	var versions []*ObjectInfo
	// Query all version objects (type=3, pid=fileID, not deleted pid>=0)
	// If excludeWriting is true, exclude writing versions (name="0")
	conds := []interface{}{b.Eq("pid", fileID), b.Eq("type", OBJ_TYPE_VERSION), "pid >= 0"}
	if excludeWriting {
		conds = append(conds, b.Neq("name", WritingVersionName))
	}
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&versions,
		b.Where(conds...),
		b.OrderBy("mtime DESC"))
	if err != nil {
		return nil, ERR_QUERY_DB
	}
	return versions, nil
}

func (dma *DefaultMetadataAdapter) DeleteObj(c Ctx, bktID int64, id int64) error {
	db, err := GetDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer db.Close()

	// Get object information
	obj := &ObjectInfo{}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(obj, b.Where(b.Eq("id", id))); err != nil {
		return ERR_QUERY_DB
	}

	// Flip object's PID to negative to mark as deleted (so it disappears from original tree structure)
	// Also update MTime to current timestamp
	// Note: If original PID is 0 (ROOT_OID), use -1 as special marker
	newPID := -obj.PID
	if newPID == 0 {
		newPID = -1 // Negative of ROOT_OID is still 0, use -1 as special marker
	}
	currentTime := Now()

	// Check if there's already a deleted object with same name in same parent directory (PID < 0 and |PID| == original PID)
	// If exists, rename current object to avoid conflict
	var conflictingObjs []*ObjectInfo
	conflictCond := []interface{}{b.Eq("pid", newPID), b.Eq("name", obj.Name), b.Neq("id", id)}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&conflictingObjs,
		b.Where(conflictCond...)); err == nil && len(conflictingObjs) > 0 {
		// Conflict exists, rename current object (add timestamp suffix)
		ext := filepath.Ext(obj.Name)
		nameWithoutExt := strings.TrimSuffix(obj.Name, ext)
		newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, currentTime, ext)
		obj.Name = newName
	}

	// Update object's PID and MTime, also update name if there's a conflict
	updateFields := []string{"pid", "mtime"}
	hasNameChange := len(conflictingObjs) > 0
	if hasNameChange {
		updateFields = append(updateFields, "name")
	}

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(&ObjectInfo{
		ID:    id,
		PID:   newPID,
		MTime: currentTime,
		Name:  obj.Name,
	}, b.Fields(updateFields...), b.Where(b.Eq("id", id))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	var objs []*ObjectInfo
	// PID < 0 or PID = -1 indicates deleted objects
	conds := []interface{}{"pid < 0 OR pid = -1"}
	if beforeTime > 0 {
		conds = append(conds, b.Lte("mtime", beforeTime))
	}

	if limit > 0 {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime"),
			b.Limit(limit)); err != nil {
			return nil, ERR_QUERY_DB
		}
	} else {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime")); err != nil {
			return nil, ERR_QUERY_DB
		}
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListRecycleBin(c Ctx, bktID int64, opt ListOptions) (o []*ObjectInfo, cnt int64, d string, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, "", ERR_OPEN_DB
	}
	defer db.Close()

	// PID < 0 或 PID = -1 表示已删除的对象
	conds := []interface{}{"pid < 0 OR pid = -1"}

	// Process filter word
	if opt.Word != "" {
		if strings.ContainsAny(opt.Word, "*?") {
			// SQLite branch uses LIKE pattern matching
			conds = append(conds, fmt.Sprintf("name LIKE '%s'", strings.ReplaceAll(strings.ReplaceAll(opt.Word, "*", "%"), "?", "_")))
		} else {
			conds = append(conds, b.Eq("name", opt.Word))
		}
	}

	// Get total count
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", ERR_QUERY_DB
	}

	if opt.Count > 0 {
		// Process sorting
		order := opt.Order
		if order == "" {
			order = "mtime"
		}
		fn := b.Gt
		orderBy := order
		switch order[0] {
		case '-':
			fn = b.Lt
			order = order[1:]
			orderBy = order + " desc"
		case '+':
			order = order[1:]
			orderBy = order
		}
		if order != "id" && order != "name" {
			orderBy = orderBy + ", id"
		}

		// Process delimiter (for pagination)
		if opt.Delim != "" {
			ds := strings.Split(opt.Delim, ":")
			if len(ds) > 0 && ds[0] != "" {
				if order == "id" || order == "name" {
					conds = append(conds, fn(order, ds[0]))
				} else if len(ds) == 2 {
					conds = append(conds, b.Or(fn(order, ds[0]),
						b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
				}
			}
		}

		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(opt.Count)); err != nil {
			return nil, 0, "", ERR_QUERY_DB
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}
