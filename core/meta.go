package core

import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	_ "github.com/mattn/go-sqlite3"
	b "github.com/orca-zhang/borm"
)

type BucketInfo struct {
	ID           int64  `borm:"id" json:"i,omitempty"`  // Bucket ID
	Name         string `borm:"n" json:"n,omitempty"`   // Bucket name
	Type         int    `borm:"t" json:"t,omitempty"`   // Bucket type, 0: none, 1: normal ...
	Quota        int64  `borm:"q" json:"q,omitempty"`   // Quota, negative means unlimited
	Used         int64  `borm:"u" json:"s,omitempty"`   // Logical usage, counts original size of all versions
	RealUsed     int64  `borm:"ru" json:"ru,omitempty"` // Actual physical usage, counts actual stored data size
	LogicalUsed  int64  `borm:"lu" json:"lu,omitempty"` // Logical occupancy, counts logical size of all valid objects (not deleted, PID >= 0) considering deduplication but excluding deleted objects
	DedupSavings int64  `borm:"ds" json:"ds,omitempty"` // Instant upload space savings, counts deduplicated data savings (LogicalUsed - unique data block size)
	ChunkSize    int64  `borm:"cs" json:"cs,omitempty"` // Chunk size (bytes), must be >0 (defaults to system chunk size)
	// SnapshotID int64 // Latest snapshot version ID
}

// BucketACL represents access control list for a bucket
type BucketACL struct {
	BktID int64 `borm:"bkt_id" json:"bkt_id,omitempty"` // Bucket ID
	UID   int64 `borm:"uid" json:"uid,omitempty"`       // User ID
	Perm  int   `borm:"perm" json:"perm,omitempty"`     // Permission flags (DR, DW, DD, MDR, MDW, MDD, or ALL)
}

type UserInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`     // User ID
	Usr    string `borm:"usr" json:"u,omitempty"`    // Username
	Pwd    string `borm:"pwd" json:"p,omitempty"`    // Password, encrypted using PBKDF2-HMAC-SHA256
	Role   uint32 `borm:"role" json:"r,omitempty"`   // User role: regular user / administrator
	Name   string `borm:"name" json:"n,omitempty"`   // Name
	Avatar string `borm:"avatar" json:"a,omitempty"` // Avatar
}

// Object types
const (
	OBJ_TYPE_MALFORMED = iota - 1
	OBJ_TYPE_NONE
	OBJ_TYPE_DIR
	OBJ_TYPE_FILE
	OBJ_TYPE_VERSION
	OBJ_TYPE_PREVIEW
)

type ObjectInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`  // Object ID (randomly generated by idgen)
	PID    int64  `borm:"pid" json:"p,omitempty"` // Parent object ID
	MTime  int64  `borm:"m" json:"m,omitempty"`   // Update time, second-level timestamp
	DataID int64  `borm:"did" json:"d,omitempty"` // Data ID, if 0, no data (newly created file, DataID is object ID, serving as first version data)
	Type   int    `borm:"t" json:"t,omitempty"`   // Object type, -1: malformed, 0: none, 1: dir, 2: file, 3: version, 4: preview(thumb/m3u8/pdf)
	Name   string `borm:"n" json:"n,omitempty"`   // Object name
	Size   int64  `borm:"s" json:"s,omitempty"`   // Object size, directory size is child object count, file size is latest version byte count
	Extra  string `borm:"e" json:"e,omitempty"`   // Object extended information
}

// Data status
const (
	DATA_NORMAL         = uint32(1 << iota) // Normal
	DATA_ENDEC_AES256                       // Whether AES encrypted
	DATA_ENDEC_SM4                          // Whether SM4 encrypted
	DATA_ENDEC_RESERVED                     // Whether reserved encryption
	DATA_CMPR_SNAPPY                        // Whether snappy compressed
	DATA_CMPR_ZSTD                          // Whether zstd compressed
	DATA_CMPR_GZIP                          // Whether gzip compressed
	DATA_CMPR_BR                            // Whether brotli compressed
	DATA_KIND_IMG                           // Image type
	DATA_KIND_VIDEO                         // Video type
	DATA_KIND_AUDIO                         // Audio type
	DATA_KIND_ARCHIVE                       // Archive type
	DATA_KIND_DOCS                          // Document type
	DATA_KIND_FONT                          // Font type
	DATA_KIND_APP                           // Application type
	DATA_KIND_RESERVED                      // Unknown type
	DATA_SPARSE                             // Sparse file: chunks may not exist, read as zeros

	DATA_MALFORMED  = 0 // Whether corrupted
	DATA_ENDEC_MASK = DATA_ENDEC_AES256 | DATA_ENDEC_SM4 | DATA_ENDEC_RESERVED
	DATA_CMPR_MASK  = DATA_CMPR_SNAPPY | DATA_CMPR_ZSTD | DATA_CMPR_GZIP | DATA_CMPR_BR
	DATA_KIND_MASK  = DATA_KIND_IMG | DATA_KIND_VIDEO | DATA_KIND_AUDIO | DATA_KIND_ARCHIVE | DATA_KIND_DOCS | DATA_KIND_FONT | DATA_KIND_APP | DATA_KIND_RESERVED
)

type DataInfo struct {
	ID        int64  `borm:"id" json:"i,omitempty"`  // Data ID (randomly generated by idgen)
	Size      int64  `borm:"s" json:"s,omitempty"`   // Data size (compressed/encrypted)
	OrigSize  int64  `borm:"os" json:"r,omitempty"`  // Original data size (before compression/encryption)
	HdrXXH3   int64  `borm:"h" json:"h,omitempty"`   // XXHash3-64bit checksum of first 100KB
	XXH3      int64  `borm:"x" json:"x,omitempty"`   // XXHash3-64bit checksum of entire original data
	SHA256_0  int64  `borm:"s0" json:"s0,omitempty"` // SHA-256 hash of entire original data (bytes 0-7)
	SHA256_1  int64  `borm:"s1" json:"s1,omitempty"` // SHA-256 hash of entire original data (bytes 8-15)
	SHA256_2  int64  `borm:"s2" json:"s2,omitempty"` // SHA-256 hash of entire original data (bytes 16-23)
	SHA256_3  int64  `borm:"s3" json:"s3,omitempty"` // SHA-256 hash of entire original data (bytes 24-31)
	Cksum     int64  `borm:"c" json:"c,omitempty"`   // Checksum of final data (for consistency audit)
	Kind      uint32 `borm:"k" json:"k,omitempty"`   // Data status: normal, corrupted, encrypted, compressed, type (for preview, etc.)
	PkgID     int64  `borm:"pi" json:"p,omitempty"`  // Package data ID (also generated by idgen)
	PkgOffset uint32 `borm:"po" json:"g,omitempty"`  // Offset position in package data
	// PkgID != 0 indicates packaged data
	// SnapshotID int64 // Snapshot version ID
}

const EmptyDataID = 4708888888888

// WritingVersionName is the special name for "writing version" that allows direct data block modification
// Versions with name="0" can be directly modified without creating new versions
const WritingVersionName = "0"

func EmptyDataInfo() *DataInfo {
	// SHA-256 of empty string: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
	return &DataInfo{
		ID:       EmptyDataID,
		SHA256_0: -2039914840885289964, // bytes 0-7
		SHA256_1: -7278955230309402332, // bytes 8-15
		SHA256_2: 2859295262623109964,  // bytes 16-23
		SHA256_3: -6587190536697628587, // bytes 24-31
		Kind:     DATA_NORMAL,
	}
}

// IsSparseFile checks if DataInfo represents a sparse file
// Sparse files have DATA_SPARSE flag set, meaning chunks may not exist and should be read as zeros
func IsSparseFile(dataInfo *DataInfo) bool {
	if dataInfo == nil {
		return false
	}
	return dataInfo.Kind&DATA_SPARSE != 0
}

// MarkSparseFile marks a DataInfo as sparse file
func MarkSparseFile(dataInfo *DataInfo) {
	if dataInfo != nil {
		dataInfo.Kind |= DATA_SPARSE
	}
}

const (
	ACL_TBL = "acl" // Access Control List table
	USR_TBL = "usr"

	BKT_TBL  = "bkt"
	OBJ_TBL  = "obj"
	DATA_TBL = "data"
)

type UserMetadataAdapter interface {
	PutUsr(c Ctx, u *UserInfo) error
	GetUsr(c Ctx, ids []int64) ([]*UserInfo, error)
	GetUsr2(c Ctx, usr string) (*UserInfo, error)
	SetUsr(c Ctx, fields []string, u *UserInfo) error
	ListUsers(c Ctx) ([]*UserInfo, error)
	DeleteUser(c Ctx, userID int64) error
}

type BucketMetadataAdapter interface {
	PutBkt(c Ctx, o []*BucketInfo) error
	DeleteBkt(c Ctx, bktID int64) error
	GetBkt(c Ctx, ids []int64) ([]*BucketInfo, error)
	ListBkt(c Ctx, uid int64) ([]*BucketInfo, error)
	ListAllBuckets(c Ctx) ([]*BucketInfo, error) // Get all buckets (for scheduled tasks)
	// Update bucket quota and usage
	UpdateBktQuota(c Ctx, bktID int64, quota int64) error
	// Increase bucket's actual usage (when uploading data)
	IncBktRealUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's actual usage (when deleting data)
	DecBktRealUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical usage (when creating objects, including instant upload)
	IncBktUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical usage (when deleting objects)
	DecBktUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical occupancy (when creating objects, only count valid objects)
	IncBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical occupancy (when deleting objects)
	DecBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's deduplication savings (saved data size from instant upload)
	IncBktDedupSavings(c Ctx, bktID int64, size int64) error
	// Decrease bucket's deduplication savings (when deleting objects)
	DecBktDedupSavings(c Ctx, bktID int64, size int64) error
}

// DuplicateGroup represents a group of duplicate data with same checksums
type DuplicateGroup struct {
	OrigSize int64   // Original data size
	HdrXXH3  int64   // XXHash3-64bit checksum of first 100KB (as int64)
	XXH3     int64   // XXHash3-64bit checksum of entire original data (as int64)
	SHA256_0 int64   // SHA-256 hash bytes 0-7
	SHA256_1 int64   // SHA-256 hash bytes 8-15
	SHA256_2 int64   // SHA-256 hash bytes 16-23
	SHA256_3 int64   // SHA-256 hash bytes 24-31
	DataIDs  []int64 // List of DataIDs with same checksums
}

// DataMetadataAdapter manages data metadata stored in bucket databases
// DataInfoMetadataAdapter manages data metadata stored in bucket databases
type DataInfoMetadataAdapter interface {
	RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error)
	PutData(c Ctx, bktID int64, d []*DataInfo) error
	GetData(c Ctx, bktID, id int64) (*DataInfo, error)
	// PutDataAndObj writes both DataInfo and ObjectInfo in a single transaction
	// This optimization reduces database round trips
	PutDataAndObj(c Ctx, bktID int64, d []*DataInfo, o []*ObjectInfo) error
	ListAllData(c Ctx, bktID int64, offset, limit int) ([]*DataInfo, int64, error) // offset: offset, limit: page size, returns data and total count
	// Find duplicate data: returns DataID groups with same checksums (OrigSize, HdrXXH3, XXH3, SHA256 all same)
	FindDuplicateData(c Ctx, bktID int64, offset, limit int) ([]DuplicateGroup, int64, error) // Returns duplicate data groups and total count
	// Update object's DataID reference
	UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error
	// Delete data metadata
	DeleteData(c Ctx, bktID int64, dataIDs []int64) error
	// Find small file data that can be packaged (for defragmentation)
	FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) ([]*DataInfo, int64, error)
}

type ObjectMetadataAdapter interface {
	PutObj(c Ctx, bktID int64, o []*ObjectInfo) ([]int64, error)
	GetObj(c Ctx, bktID int64, ids []int64) ([]*ObjectInfo, error)
	SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error
	ListObj(c Ctx, bktID, pid int64, wd, delim, order string, count int) ([]*ObjectInfo, int64, string, error)
	CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error)               // Count DataID references
	DeleteObj(c Ctx, bktID int64, id int64) error                                             // Delete object (mark as deleted, flip PID to negative)
	ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error)   // List deleted objects (PID < 0)
	ListRecycleBin(c Ctx, bktID int64, opt ListOptions) ([]*ObjectInfo, int64, string, error) // List recycle bin (objects with PID < 0)
	// Query all objects of specified type (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all child objects under specified directory (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all objects that reference the specified DataID
	GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error)
	// Query all versions of a file (sorted by MTime descending, latest first)
	// excludeWriting: if true, exclude writing versions (name="0")
	ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error)
}

// ACLMetadataManager manages access control lists for buckets (many-to-many relationship)
type ACLMetadataManager interface {
	// PutACL adds or updates an ACL entry (bktID, uid pair) with permission
	// perm: permission flags (DR, DW, DD, MDR, MDW, MDD, or ALL)
	PutACL(c Ctx, bktID int64, uid int64, perm int) error
	// ListACL lists all ACL entries for a bucket
	ListACL(c Ctx, bktID int64) ([]*BucketACL, error)
	// DeleteACL deletes a specific ACL entry (bktID, uid pair)
	DeleteACL(c Ctx, bktID int64, uid int64) error
	// DeleteAllACL deletes all ACL entries for a bucket
	DeleteAllACL(c Ctx, bktID int64) error
	// ListACLByUser lists all buckets accessible by a user
	ListACLByUser(c Ctx, uid int64) ([]*BucketACL, error)
	// CheckPermission checks if a user has the required permission for a bucket
	// Returns true if the user's ACL permission covers the required action
	CheckPermission(c Ctx, bktID int64, uid int64, action int) (bool, error)
}

// BaseMetadataAdapter manages base metadata (users, buckets, ACL) stored in main database
type BaseMetadataAdapter interface {
	Close()
	UserMetadataAdapter
	ACLMetadataManager
}

// DataMetadataAdapter manages data and object metadata stored in bucket databases
type DataMetadataAdapter interface {
	Close()
	BucketMetadataAdapter
	DataInfoMetadataAdapter
	ObjectMetadataAdapter
}

// MetadataAdapter combines BaseMetadataAdapter and DataMetadataAdapter for backward compatibility
type MetadataAdapter interface {
	Close()
	BaseMetadataAdapter
	DataMetadataAdapter
}

// EscapeSQLString 转义SQL字符串，只遍历一次字符串
// 将单引号 ' 转义为 ”，防止SQL注入
func EscapeSQLString(s string) string {
	// 先检查是否包含需要转义的字符，避免不必要的内存分配
	if !strings.Contains(s, "'") {
		return s
	}

	// 预估容量：每个单引号会变成两个，所以最多需要2倍长度
	var builder strings.Builder
	builder.Grow(len(s) + strings.Count(s, "'"))

	// 只遍历一次字符串
	for _, r := range s {
		if r == '\'' {
			builder.WriteString("''")
		} else {
			builder.WriteRune(r)
		}
	}

	return builder.String()
}

func GetDB(c ...interface{}) (*sql.DB, error) {
	// Use proper parameters for concurrent access:
	// - _journal=WAL: Write-Ahead Logging for better concurrency
	// - cache=shared: Share cache between connections
	// - mode=rwc: Read-Write-Create mode
	// - _busy_timeout=10000: Wait up to 10 seconds for locks (increased for high concurrency)
	// - _txlock=immediate: Use immediate transaction locks to reduce contention
	param := "?_journal=WAL&cache=shared&mode=rwc&_busy_timeout=10000&_txlock=immediate"
	var dirPath string
	var dbKey string
	var ctx Ctx

	if len(c) > 0 {
		// Check if first parameter is a string (database key)
		if keyStr, ok := c[0].(string); ok {
			dbKey = keyStr
			c = c[1:] // Remove key from remaining params
		} else if ctxVal, ok := c[0].(Ctx); ok {
			// If it's a Ctx, try to get key from context
			ctx = ctxVal
			if key := getKey(ctx); key != "" {
				dbKey = key
			}
			c = c[1:] // Remove context from remaining params
		} else if ctxVal, ok := c[0].(context.Context); ok {
			// Handle standard context.Context (shouldn't happen, but be safe)
			// Try to convert to Ctx and extract key
			ctx = Ctx(ctxVal)
			if key := getKey(ctx); key != "" {
				dbKey = key
			}
			c = c[1:] // Remove context from remaining params
		}
	}

	// Get base path from context or use global variable
	dirPath = getBasePath(ctx)

	if len(c) > 0 {
		// Next parameter should be bucket ID (int64)
		// Ensure it's actually an int64, not accidentally a context
		if bktID, ok := c[0].(int64); ok {
			// Get data path from context or use global variable
			dataPath := getDataPath(ctx)
			dirPath = filepath.Join(dataPath, fmt.Sprint(bktID))
		} else {
			// Invalid parameter type - this should not happen
			// Log error but try to continue with default path
			return nil, fmt.Errorf("invalid bucket ID type: expected int64, got %T", c[0])
		}
		if len(c) > 1 {
			if ctx, ok := c[1].(Ctx); ok {
				if key := getKey(ctx); key != "" {
					dbKey = key
				}
			}
		}
	}

	if dbKey != "" {
		param += "&key=" + dbKey
	}

	os.MkdirAll(dirPath, 0o766)
	db, err := sql.Open("sqlite3", filepath.Join(dirPath, "meta.db")+param)
	if err != nil {
		return nil, err
	}

	// Set connection pool limits for better concurrency
	// MaxOpenConns = 25: Allow up to 25 concurrent connections (good for 10 concurrent clients)
	// MaxIdleConns = 10: Keep 10 connections in pool for reuse
	// ConnMaxLifetime = 0: Connections don't expire (reuse indefinitely)
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(0)

	return db, nil
}

// GetDBWithKey opens database with specified encryption key
// Requires ORCAS_BASE to be set, returns error if ORCAS_BASE is empty
func GetDBWithKey(key string) (*sql.DB, error) {
	if ORCAS_BASE == "" {
		return nil, fmt.Errorf("ORCAS_BASE is not set, main database is not available")
	}
	param := "?_journal=WAL&cache=shared&mode=rwc&_busy_timeout=10000&_txlock=immediate"
	if key != "" {
		param += "&key=" + key
	}
	dirPath := ORCAS_BASE
	os.MkdirAll(dirPath, 0o766)
	db, err := sql.Open("sqlite3", filepath.Join(dirPath, "meta.db")+param)
	if err != nil {
		return nil, err
	}

	// Set connection pool limits
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(0)

	return db, nil
}


// InitDB initializes the main database
// If key is provided, the database will be encrypted with that key
// If key is empty string, the database will be unencrypted
// Requires ORCAS_BASE to be set, returns error if ORCAS_BASE is empty
func InitDB(key ...string) error {
	if ORCAS_BASE == "" {
		return fmt.Errorf("ORCAS_BASE is not set, cannot initialize main database")
	}
	var dbKey string
	if len(key) > 0 {
		dbKey = key[0]
	}

	// Initialize connection pool if not already initialized
	InitDBPool(10, 5, 5, 0)

	// For initialization, use write connection
	db, err := GetWriteDB()
	if err != nil {
		// Fallback to old method if pool not available
		db, err = GetDBWithKey(dbKey)
		if err != nil {
			return ERR_OPEN_DB
		}
		defer db.Close()
	}
	// Note: If using pool, don't close the connection
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS usr (id BIGINT PRIMARY KEY NOT NULL,
		role TINYINT NOT NULL,
		usr TEXT NOT NULL,
		pwd TEXT NOT NULL,
		name TEXT NOT NULL,
		avatar TEXT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create usr table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_usr on usr (usr)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_usr: %v", ERR_EXEC_DB, err)
	}

	// Create ACL table in main database (for global ACL management/indexing)
	// Note: Each bucket database also has its own ACL table
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS acl (
		bkt_id BIGINT NOT NULL,
		uid BIGINT NOT NULL,
		perm INTEGER NOT NULL DEFAULT 0,
		PRIMARY KEY (bkt_id, uid)
	)`)
	if err != nil {
		return fmt.Errorf("%w: create acl table: %v", ERR_EXEC_DB, err)
	}
	// Create indexes for efficient querying
	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_acl_uid ON acl (uid)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_acl_uid: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_acl_bkt_id ON acl (bkt_id)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_acl_bkt_id: %v", ERR_EXEC_DB, err)
	}

	// Create default admin user if no admin exists
	initDefaultAdmin(db)

	return nil
}

// initDefaultAdmin creates a default admin user if no admin user exists
func initDefaultAdmin(db *sql.DB) {
	// Check if any admin user exists
	var count int64
	ctx := context.Background()
	if _, err := b.TableContext(ctx, db, USR_TBL).Select(&count,
		b.Fields("count(1)"),
		b.Where(b.Eq("role", ADMIN))); err != nil {
		return // If query fails, skip creating default admin
	}

	// If no admin exists, create default admin user
	if count == 0 {
		// Default admin credentials: username "orcas", password "orcas"
		hashedPwd, err := HashPassword("orcas")
		if err != nil {
			return // If password hashing fails, skip creating default admin
		}

		// Generate a new ID for the admin user
		adminID := NewID()
		if adminID <= 0 {
			return // If ID generation fails, skip creating default admin
		}

		// Create default admin user
		adminUser := &UserInfo{
			ID:     adminID,
			Role:   ADMIN,
			Usr:    "orcas",
			Pwd:    hashedPwd,
			Name:   "Administrator",
			Avatar: "",
		}
		if _, err = b.TableContext(ctx, db, USR_TBL).Insert(adminUser); err != nil {
			// If insert fails (e.g., user already exists), ignore the error
			return
		}
	}
}

func InitBucketDB(c Ctx, bktID int64) error {
	// Initialize connection pool if not already initialized
	InitDBPool(10, 5, 5, 0)

	// For initialization, use write connection
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		// Fallback to old method if pool not available
		db, err = GetDB(c, bktID)
		if err != nil {
			return fmt.Errorf("%w: %v", ERR_OPEN_DB, err)
		}
		defer db.Close()
	}
	// Note: If using pool, don't close the connection

	// Create bkt table in bucket database (bucket info is now stored in bucket database)
	// Note: UID is removed from bkt table, stored in ACL table instead
	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS bkt (id BIGINT PRIMARY KEY NOT NULL,
		q BIGINT NOT NULL,
		u BIGINT NOT NULL,
		ru BIGINT NOT NULL DEFAULT 0,
		lu BIGINT NOT NULL DEFAULT 0,
		ds BIGINT NOT NULL DEFAULT 0,
		t TINYINT NOT NULL,
		n TEXT NOT NULL,
		cs BIGINT NOT NULL DEFAULT 0
	)`)
	if err != nil {
		return fmt.Errorf("%w: create bkt table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS obj (id BIGINT PRIMARY KEY NOT NULL,
		pid BIGINT NOT NULL,
		did BIGINT NOT NULL,
		s BIGINT NOT NULL,
		m BIGINT NOT NULL,
		t TINYINT NOT NULL,
		n TEXT NOT NULL,
		e TEXT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create obj table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS data (id BIGINT PRIMARY KEY NOT NULL,
		s BIGINT NOT NULL,
		os BIGINT NOT NULL,
		s0 BIGINT NOT NULL,
		s1 BIGINT NOT NULL,
		s2 BIGINT NOT NULL,
		s3 BIGINT NOT NULL,
		pi BIGINT NOT NULL,
		po INTEGER NOT NULL,
		h BIGINT NOT NULL,
		x BIGINT NOT NULL,
		c BIGINT NOT NULL,
		k SMALLINT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create data table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_pid_name on obj (pid, n)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_pid_name: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_ref ON data (os, h, x, s0, s1, s2, s3)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_ref: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`PRAGMA temp_store = MEMORY`)
	if err != nil {
		return fmt.Errorf("%w: set pragma: %v", ERR_EXEC_DB, err)
	}
	return nil
}

// DefaultBaseMetadataAdapter implements BaseMetadataAdapter
type DefaultBaseMetadataAdapter struct{}

func (dba *DefaultBaseMetadataAdapter) Close() {
}

// DefaultDataMetadataAdapter implements DataMetadataAdapter
type DefaultDataMetadataAdapter struct{}

func (dda *DefaultDataMetadataAdapter) Close() {
}

// DefaultMetadataAdapter combines BaseMetadataAdapter and DataMetadataAdapter
// for backward compatibility
type DefaultMetadataAdapter struct {
	*DefaultBaseMetadataAdapter
	*DefaultDataMetadataAdapter
}

func NewDefaultMetadataAdapter() *DefaultMetadataAdapter {
	return &DefaultMetadataAdapter{
		DefaultBaseMetadataAdapter: &DefaultBaseMetadataAdapter{},
		DefaultDataMetadataAdapter: &DefaultDataMetadataAdapter{},
	}
}

func (dma *DefaultMetadataAdapter) Close() {
	// Both adapters are embedded, no need to close separately
}

// BaseMetadataAdapter implementation (ACL, User, Bucket)

// ACLMetadataManager implementation
// ACL is stored in main database, not in bucket database
func (dba *DefaultBaseMetadataAdapter) PutACL(c Ctx, bktID int64, uid int64, perm int) error {
	// Use write connection for main database
	db, err := GetWriteDB()
	if err != nil {
		return fmt.Errorf("%w: PutACL GetWriteDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	// Default to ALL if perm is 0 or invalid
	if perm == 0 {
		perm = ALL
	}

	acl := &BucketACL{BktID: bktID, UID: uid, Perm: perm}
	aclSlice := []*BucketACL{acl}
	// Use ReplaceInto to insert or replace (since we have composite primary key)
	if _, err = b.TableContext(c, db, ACL_TBL).ReplaceInto(&aclSlice); err != nil {
		return fmt.Errorf("%w: PutACL failed (bktID=%d, uid=%d, perm=%d): %v", ERR_EXEC_DB, bktID, uid, perm, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) ListACL(c Ctx, bktID int64) ([]*BucketACL, error) {
	// Use read connection for main database
	db, err := GetReadDB()
	if err != nil {
		return nil, fmt.Errorf("%w: ListACL GetReadDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	var acls []*BucketACL
	if _, err = b.TableContext(c, db, ACL_TBL).Select(&acls, b.Where(b.Eq("bkt_id", bktID))); err != nil {
		return nil, fmt.Errorf("%w: ListACL failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
	}
	return acls, nil
}

func (dba *DefaultBaseMetadataAdapter) DeleteACL(c Ctx, bktID int64, uid int64) error {
	// Use write connection for main database
	db, err := GetWriteDB()
	if err != nil {
		return fmt.Errorf("%w: DeleteACL GetWriteDB failed (bktID=%d, uid=%d): %v", ERR_OPEN_DB, bktID, uid, err)
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, ACL_TBL).Delete(b.Where(b.Eq("bkt_id", bktID), b.Eq("uid", uid))); err != nil {
		return fmt.Errorf("%w: DeleteACL failed (bktID=%d, uid=%d): %v", ERR_EXEC_DB, bktID, uid, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) DeleteAllACL(c Ctx, bktID int64) error {
	// Use write connection for main database
	db, err := GetWriteDB()
	if err != nil {
		return fmt.Errorf("%w: DeleteAllACL GetWriteDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, ACL_TBL).Delete(b.Where(b.Eq("bkt_id", bktID))); err != nil {
		return fmt.Errorf("%w: DeleteAllACL failed (bktID=%d): %v", ERR_EXEC_DB, bktID, err)
	}
	return nil
}

// CheckPermission checks if a user has the required permission for a bucket
// Returns true if the user's ACL permission covers the required action
func (dba *DefaultBaseMetadataAdapter) CheckPermission(c Ctx, bktID int64, uid int64, action int) (bool, error) {
	// Use read connection for main database
	db, err := GetReadDB()
	if err != nil {
		return false, fmt.Errorf("%w: CheckPermission GetReadDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	var acls []*BucketACL
	if _, err = b.TableContext(c, db, ACL_TBL).Select(&acls, b.Where(b.Eq("bkt_id", bktID), b.Eq("uid", uid))); err != nil {
		return false, fmt.Errorf("%w: CheckPermission failed (bktID=%d, uid=%d): %v", ERR_QUERY_DB, bktID, uid, err)
	}
	if len(acls) == 0 {
		return false, nil
	}

	// Check if any ACL entry has permission that covers the required action
	// Permission covers action if (perm & action) == action
	for _, acl := range acls {
		if (acl.Perm & action) == action {
			return true, nil
		}
	}
	return false, nil
}

func (dba *DefaultBaseMetadataAdapter) ListACLByUser(c Ctx, uid int64) ([]*BucketACL, error) {
	// ACL is stored in main database, query directly
	db, err := GetReadDB()
	if err != nil {
		return nil, fmt.Errorf("%w: ListACLByUser GetReadDB failed (uid=%d): %v", ERR_OPEN_DB, uid, err)
	}
	// Note: Don't close the connection, it's from the pool

	var acls []*BucketACL
	if _, err = b.TableContext(c, db, ACL_TBL).Select(&acls, b.Where(b.Eq("uid", uid))); err != nil {
		return nil, fmt.Errorf("%w: ListACLByUser failed (uid=%d): %v", ERR_QUERY_DB, uid, err)
	}
	// Sort results by BktID to ensure consistent ordering
	sort.Slice(acls, func(i, j int) bool {
		return acls[i].BktID < acls[j].BktID
	})
	return acls, nil
}

func (dma *DefaultMetadataAdapter) RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error) {
	// Use write connection because we need to create temporary table
	// Temporary tables require write access
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	tbl := fmt.Sprintf("tmp_%x", time.Now().UnixNano())
	// 创建临时表
	if _, err = db.Exec(`CREATE TEMPORARY TABLE ` + tbl + ` (os BIGINT NOT NULL,
		h BIGINT NOT NULL,
		x BIGINT NOT NULL,
		s0 BIGINT NOT NULL,
		s1 BIGINT NOT NULL,
		s2 BIGINT NOT NULL,
		s3 BIGINT NOT NULL
	)`); err != nil {
		return nil, fmt.Errorf("%w: create temp table: %v", ERR_EXEC_DB, err)
	}
	// 把待查询数据放到临时表（使用 INSERT OR IGNORE 避免重复数据导致的错误）
	// DataInfo 中的 HdrXXH3 和 XXH3 已经是 int64 类型，可以直接使用
	if _, err = b.TableContext(c, db, tbl).InsertIgnore(&d,
		b.Fields("os", "h", "x", "s0", "s1", "s2", "s3")); err != nil {
		return nil, fmt.Errorf("%w: insert into temp table: %v", ERR_EXEC_DB, err)
	}
	var refs []struct {
		ID       int64 `borm:"max(a.id)"`
		OrigSize int64 `borm:"b.os"`
		HdrXXH3  int64 `borm:"b.h"`
		XXH3     int64 `borm:"b.x"`
		SHA256_0 int64 `borm:"b.s0"`
		SHA256_1 int64 `borm:"b.s1"`
		SHA256_2 int64 `borm:"b.s2"`
		SHA256_3 int64 `borm:"b.s3"`
	}
	// 联表查询
	if _, err = b.TableContext(c, db, `data a, `+tbl+` b`).Select(&refs,
		b.Join(`on a.os=b.os and a.h=b.h and 
			(b.x=0 or b.s0=0 or (a.x=b.x and a.s0=b.s0 and a.s1=b.s1 and a.s2=b.s2 and a.s3=b.s3))`),
		b.GroupBy("b.os", "b.h", "b.x", "b.s0", "b.s1", "b.s2", "b.s3")); err != nil {
		return nil, fmt.Errorf("%w: RefData join query failed (bktID=%d, tempTable=%s): %v", ERR_QUERY_DB, bktID, tbl, err)
	}
	// 删除临时表
	db.Exec(`DROP TABLE ` + tbl)

	// 构造辅助查询map
	aux := make(map[string]int64, 0)
	for _, ref := range refs {
		aux[fmt.Sprintf("%d:%d:%d:%d:%d:%d:%d", ref.OrigSize, ref.HdrXXH3, ref.XXH3, ref.SHA256_0, ref.SHA256_1, ref.SHA256_2, ref.SHA256_3)] = ref.ID
	}

	res := make([]int64, len(d))
	for i, x := range d {
		// 如果最基础的数据不完整，直接跳过
		if x.OrigSize == 0 || x.HdrXXH3 == 0 {
			continue
		}

		key := fmt.Sprintf("%d:%d:%d:%d:%d:%d:%d", x.OrigSize, x.HdrXXH3, x.XXH3, x.SHA256_0, x.SHA256_1, x.SHA256_2, x.SHA256_3)
		if id, ok := aux[key]; ok {
			// Found in database or current batch
			if id < 0 {
				// Negative ID means reference to another element in current batch (using two's complement)
				// Return the negative index directly (^index format)
				res[i] = id
			} else {
				// Positive ID means found in database
				// 全文件的数据没有，说明是预Ref
				if x.XXH3 == 0 || x.SHA256_0 == 0 {
					if id > 0 {
						res[i] = 1 // 非0代表预Ref成功，预Ref只看数据库
					}
				} else {
					res[i] = id
				}
			}
		} else {
			// 没有秒传成功，但是当前批次可能有一样的数据
			// Store negative index (^i) for future elements in this batch to reference
			aux[key] = int64(^i)
		}
	}
	return res, nil
}

func (dma *DefaultMetadataAdapter) PutData(c Ctx, bktID int64, d []*DataInfo) error {
	// Use write connection for data insertion
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Convert uint64 fields to int64 for SQLite compatibility
	// SQLite's go-sqlite3 driver doesn't support uint64 values with high bit set
	type DataInfoForDB struct {
		ID        int64  `borm:"id"`
		Size      int64  `borm:"s"`
		OrigSize  int64  `borm:"os"`
		HdrXXH3   int64  `borm:"h"` // Convert uint64 to int64
		XXH3      int64  `borm:"x"` // Convert uint64 to int64
		SHA256_0  int64  `borm:"s0"`
		SHA256_1  int64  `borm:"s1"`
		SHA256_2  int64  `borm:"s2"`
		SHA256_3  int64  `borm:"s3"`
		Cksum     int64  `borm:"c"` // Convert uint64 to int64
		Kind      uint32 `borm:"k"`
		PkgID     int64  `borm:"pi"`
		PkgOffset uint32 `borm:"po"`
	}

	dataForDB := make([]*DataInfoForDB, len(d))
	for i, di := range d {
		dataForDB[i] = &DataInfoForDB{
			ID:        di.ID,
			Size:      di.Size,
			OrigSize:  di.OrigSize,
			HdrXXH3:   int64(di.HdrXXH3),
			XXH3:      int64(di.XXH3),
			SHA256_0:  di.SHA256_0,
			SHA256_1:  di.SHA256_1,
			SHA256_2:  di.SHA256_2,
			SHA256_3:  di.SHA256_3,
			Cksum:     int64(di.Cksum),
			Kind:      di.Kind,
			PkgID:     di.PkgID,
			PkgOffset: di.PkgOffset,
		}
	}

	if _, err = b.TableContext(c, db, DATA_TBL).ReplaceInto(&dataForDB); err != nil {
		return fmt.Errorf("%w: PutData failed (bktID=%d, count=%d): %v", ERR_EXEC_DB, bktID, len(d), err)
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetData(c Ctx, bktID, id int64) (d *DataInfo, err error) {
	// Use read connection for data retrieval
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Use intermediate struct to handle uint64 conversion
	type DataInfoFromDB struct {
		ID        int64  `borm:"id"`
		Size      int64  `borm:"s"`
		OrigSize  int64  `borm:"os"`
		HdrXXH3   int64  `borm:"h"` // Read as int64, convert to uint64
		XXH3      int64  `borm:"x"` // Read as int64, convert to uint64
		SHA256_0  int64  `borm:"s0"`
		SHA256_1  int64  `borm:"s1"`
		SHA256_2  int64  `borm:"s2"`
		SHA256_3  int64  `borm:"s3"`
		Cksum     int64  `borm:"c"` // Read as int64, convert to uint64
		Kind      uint32 `borm:"k"`
		PkgID     int64  `borm:"pi"`
		PkgOffset uint32 `borm:"po"`
	}

	diFromDB := &DataInfoFromDB{}
	if _, err = b.TableContext(c, db, DATA_TBL).Select(diFromDB, b.Where(b.Eq("id", id))); err != nil {
		return nil, fmt.Errorf("%w: GetData failed (bktID=%d, dataID=%d): %v", ERR_QUERY_DB, bktID, id, err)
	}

	// Convert back to DataInfo
	d = &DataInfo{
		ID:        diFromDB.ID,
		Size:      diFromDB.Size,
		OrigSize:  diFromDB.OrigSize,
		HdrXXH3:   diFromDB.HdrXXH3,
		XXH3:      diFromDB.XXH3,
		SHA256_0:  diFromDB.SHA256_0,
		SHA256_1:  diFromDB.SHA256_1,
		SHA256_2:  diFromDB.SHA256_2,
		SHA256_3:  diFromDB.SHA256_3,
		Cksum:     diFromDB.Cksum,
		Kind:      diFromDB.Kind,
		PkgID:     diFromDB.PkgID,
		PkgOffset: diFromDB.PkgOffset,
	}
	return
}

func (dma *DefaultMetadataAdapter) ListAllData(c Ctx, bktID int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	// Use read connection for listing data
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&total, b.Fields("count(1)")); err != nil {
		return nil, 0, fmt.Errorf("%w: ListAllData count failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
	}

	// Use intermediate struct to handle uint64 conversion
	type DataInfoFromDB struct {
		ID        int64  `borm:"id"`
		Size      int64  `borm:"s"`
		OrigSize  int64  `borm:"os"`
		HdrXXH3   int64  `borm:"h"`
		XXH3      int64  `borm:"x"`
		SHA256_0  int64  `borm:"s0"`
		SHA256_1  int64  `borm:"s1"`
		SHA256_2  int64  `borm:"s2"`
		SHA256_3  int64  `borm:"s3"`
		Cksum     int64  `borm:"c"`
		Kind      uint32 `borm:"k"`
		PkgID     int64  `borm:"pi"`
		PkgOffset uint32 `borm:"po"`
	}

	var dFromDB []*DataInfoFromDB

	// Paginated query: use borm to implement LIMIT and OFFSET
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&dFromDB,
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, fmt.Errorf("%w: ListAllData select failed (bktID=%d, offset=%d, limit=%d): %v", ERR_QUERY_DB, bktID, offset, limit, err)
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		if _, err = b.TableContext(c, db, DATA_TBL).Select(&dFromDB); err != nil {
			return nil, 0, fmt.Errorf("%w: ListAllData select all failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
		}
	}

	// Convert back to DataInfo
	d = make([]*DataInfo, len(dFromDB))
	for i, di := range dFromDB {
		d[i] = &DataInfo{
			ID:        di.ID,
			Size:      di.Size,
			OrigSize:  di.OrigSize,
			HdrXXH3:   di.HdrXXH3,
			XXH3:      di.XXH3,
			SHA256_0:  di.SHA256_0,
			SHA256_1:  di.SHA256_1,
			SHA256_2:  di.SHA256_2,
			SHA256_3:  di.SHA256_3,
			Cksum:     di.Cksum,
			Kind:      di.Kind,
			PkgID:     di.PkgID,
			PkgOffset: di.PkgOffset,
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) FindDuplicateData(c Ctx, bktID int64, offset, limit int) (groups []DuplicateGroup, total int64, err error) {
	// Use read connection for duplicate data search
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 定义结构体来接收GROUP BY的结果
	type duplicateGroupResult struct {
		OrigSize int64  `borm:"os"`
		HdrXXH3  uint64 `borm:"h"`
		XXH3     uint64 `borm:"x"`
		SHA256_0 int64  `borm:"s0"`
		SHA256_1 int64  `borm:"s1"`
		SHA256_2 int64  `borm:"s2"`
		SHA256_3 int64  `borm:"s3"`
		IDs      string `borm:"GROUP_CONCAT(id)"`
		Count    int64  `borm:"count(1)"`
		MinID    int64  `borm:"min(id)"`
	}

	// 查询条件
	whereConds := []interface{}{
		b.Gt("os", 0),
		b.Gt("h", 0),
		b.Gt("x", 0),
		b.Neq("s0", 0),
	}

	// 先查询所有分组（包括重复的和不重复的）
	var allGroups []duplicateGroupResult
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&allGroups,
		b.Fields("os", "h", "x", "s0", "s1", "s2", "s3", "GROUP_CONCAT(id)", "count(1)", "min(id)"),
		b.Where(whereConds...),
		b.GroupBy("os", "h", "x", "s0", "s1", "s2", "s3"),
		b.OrderBy("min(id)")); err != nil {
		return nil, 0, fmt.Errorf("%w: FindDuplicateData query failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
	}

	// 过滤出重复的组（count > 1）
	var duplicateGroups []duplicateGroupResult
	for _, g := range allGroups {
		if g.Count > 1 {
			duplicateGroups = append(duplicateGroups, g)
		}
	}

	// 计算总数
	total = int64(len(duplicateGroups))
	if total == 0 {
		return groups, 0, nil
	}

	// 分页处理
	start := offset
	end := offset + limit
	if limit <= 0 {
		end = len(duplicateGroups)
	}
	if start > len(duplicateGroups) {
		return groups, total, nil
	}
	if end > len(duplicateGroups) {
		end = len(duplicateGroups)
	}

	// 转换为DuplicateGroup
	for _, g := range duplicateGroups[start:end] {
		// 解析ID列表
		idStrs := strings.Split(g.IDs, ",")
		dataIDs := make([]int64, 0, len(idStrs))
		for _, idStr := range idStrs {
			id, err2 := strconv.ParseInt(strings.TrimSpace(idStr), 10, 64)
			if err2 != nil {
				continue
			}
			dataIDs = append(dataIDs, id)
		}

		if len(dataIDs) > 1 {
			groups = append(groups, DuplicateGroup{
				OrigSize: g.OrigSize,
				HdrXXH3:  int64(g.HdrXXH3),
				XXH3:     int64(g.XXH3),
				SHA256_0: g.SHA256_0,
				SHA256_1: g.SHA256_1,
				SHA256_2: g.SHA256_2,
				SHA256_3: g.SHA256_3,
				DataIDs:  dataIDs,
			})
		}
	}

	return
}

func (dma *DefaultMetadataAdapter) UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error {
	// Use write connection for update operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 更新所有引用oldDataID的对象，将其DataID改为newDataID
	// 使用borm进行批量更新
	updateObj := &ObjectInfo{
		DataID: newDataID,
	}
	if _, err = b.TableContext(c, db, OBJ_TBL).Update(updateObj,
		b.Fields("did"),
		b.Where(b.Eq("did", oldDataID))); err != nil {
		return fmt.Errorf("%w: UpdateObjDataID failed (bktID=%d, oldDataID=%d, newDataID=%d): %v", ERR_EXEC_DB, bktID, oldDataID, newDataID, err)
	}
	return nil
}

func (dma *DefaultMetadataAdapter) DeleteData(c Ctx, bktID int64, dataIDs []int64) error {
	if len(dataIDs) == 0 {
		return nil
	}
	// Use write connection for delete operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 批量删除数据元信息
	_, err = b.TableContext(c, db, DATA_TBL).Delete(b.Where(b.In("id", dataIDs)))
	if err != nil {
		return fmt.Errorf("%w: DeleteData failed (bktID=%d, dataIDsCount=%d): %v", ERR_EXEC_DB, bktID, len(dataIDs), err)
	}
	return nil
}

func (dma *DefaultMetadataAdapter) FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	// Use read connection for query operation
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Find small file data: pkg_id=0 (not packaged), o_size < maxSize
	// Get total count using borm
	conds := []interface{}{
		b.Eq("pi", 0),
		b.Gt("os", 0),
		b.Lt("os", maxSize),
	}
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&total,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, fmt.Errorf("%w: FindSmallPackageData count failed (bktID=%d, maxSize=%d): %v", ERR_QUERY_DB, bktID, maxSize, err)
	}

	// Use intermediate struct to handle uint64 conversion
	type DataInfoFromDB struct {
		ID        int64  `borm:"id"`
		Size      int64  `borm:"s"`
		OrigSize  int64  `borm:"os"`
		HdrXXH3   int64  `borm:"h"`
		XXH3      int64  `borm:"x"`
		SHA256_0  int64  `borm:"s0"`
		SHA256_1  int64  `borm:"s1"`
		SHA256_2  int64  `borm:"s2"`
		SHA256_3  int64  `borm:"s3"`
		Cksum     int64  `borm:"c"`
		Kind      uint32 `borm:"k"`
		PkgID     int64  `borm:"pi"`
		PkgOffset uint32 `borm:"po"`
	}

	var dFromDB []*DataInfoFromDB

	// Paginated query: use borm to implement LIMIT and OFFSET
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&dFromDB,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, fmt.Errorf("%w: FindSmallPackageData select failed (bktID=%d, maxSize=%d, offset=%d, limit=%d): %v", ERR_QUERY_DB, bktID, maxSize, offset, limit, err)
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		_, err = b.TableContext(c, db, DATA_TBL).Select(&dFromDB,
			b.Where(conds...),
			b.OrderBy("id"))
		if err != nil {
			return nil, 0, fmt.Errorf("%w: FindSmallPackageData select all failed (bktID=%d, maxSize=%d): %v", ERR_QUERY_DB, bktID, maxSize, err)
		}
	}

	// Convert back to DataInfo
	d = make([]*DataInfo, len(dFromDB))
	for i, di := range dFromDB {
		d[i] = &DataInfo{
			ID:        di.ID,
			Size:      di.Size,
			OrigSize:  di.OrigSize,
			HdrXXH3:   di.HdrXXH3,
			XXH3:      di.XXH3,
			SHA256_0:  di.SHA256_0,
			SHA256_1:  di.SHA256_1,
			SHA256_2:  di.SHA256_2,
			SHA256_3:  di.SHA256_3,
			Cksum:     di.Cksum,
			Kind:      di.Kind,
			PkgID:     di.PkgID,
			PkgOffset: di.PkgOffset,
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) PutObj(c Ctx, bktID int64, o []*ObjectInfo) (ids []int64, err error) {
	// Use write connection for object insertion
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	for _, x := range o {
		ids = append(ids, x.ID)
	}

	var n int
	if n, err = b.TableContext(c, db, OBJ_TBL).InsertIgnore(&o); err != nil {
		return nil, fmt.Errorf("%w: PutObj InsertIgnore failed (bktID=%d, count=%d): %v", ERR_EXEC_DB, bktID, len(o), err)
	}
	if n != len(o) {
		var inserted []int64
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&inserted, b.Fields("id"), b.Where(b.In("id", ids))); err != nil {
			return nil, fmt.Errorf("%w: PutObj select inserted failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
		}
		// 处理有冲突的情况
		m := make(map[int64]struct{}, 0)
		for _, v := range inserted {
			m[v] = struct{}{}
		}
		// 擦除没有插入成功的id
		for i, id := range ids {
			if _, ok := m[id]; !ok {
				ids[i] = 0
			}
		}
	}
	return
}

// PutDataAndObj writes both DataInfo and ObjectInfo in a single transaction
// This optimization reduces database round trips by combining two separate writes
func (dma *DefaultMetadataAdapter) PutDataAndObj(c Ctx, bktID int64, d []*DataInfo, o []*ObjectInfo) error {
	// Use write connection for combined write operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Use transaction to ensure atomicity
	tx, err := db.Begin()
	if err != nil {
		return fmt.Errorf("%w: PutObjAndData Begin transaction failed (bktID=%d): %v", ERR_EXEC_DB, bktID, err)
	}
	defer tx.Rollback()

	// Write DataInfo first
	if len(d) > 0 {
		// Convert uint64 fields to int64 for SQLite compatibility
		type DataInfoForDB struct {
			ID        int64  `borm:"id"`
			Size      int64  `borm:"s"`
			OrigSize  int64  `borm:"os"`
			HdrXXH3   int64  `borm:"h"` // Convert uint64 to int64
			XXH3      int64  `borm:"x"` // Convert uint64 to int64
			SHA256_0  int64  `borm:"s0"`
			SHA256_1  int64  `borm:"s1"`
			SHA256_2  int64  `borm:"s2"`
			SHA256_3  int64  `borm:"s3"`
			Cksum     int64  `borm:"c"` // Convert uint64 to int64
			Kind      uint32 `borm:"k"`
			PkgID     int64  `borm:"pi"`
			PkgOffset uint32 `borm:"po"`
		}

		dataForDB := make([]*DataInfoForDB, len(d))
		for i, di := range d {
			dataForDB[i] = &DataInfoForDB{
				ID:        di.ID,
				Size:      di.Size,
				OrigSize:  di.OrigSize,
				HdrXXH3:   int64(di.HdrXXH3),
				XXH3:      int64(di.XXH3),
				SHA256_0:  di.SHA256_0,
				SHA256_1:  di.SHA256_1,
				SHA256_2:  di.SHA256_2,
				SHA256_3:  di.SHA256_3,
				Cksum:     int64(di.Cksum),
				Kind:      di.Kind,
				PkgID:     di.PkgID,
				PkgOffset: di.PkgOffset,
			}
		}

		if _, err = b.TableContext(c, tx, DATA_TBL).ReplaceInto(&dataForDB); err != nil {
			return fmt.Errorf("%w: PutObjAndData ReplaceInto DATA failed (bktID=%d, dataCount=%d): %v", ERR_EXEC_DB, bktID, len(d), err)
		}
	}

	// Write ObjectInfo
	if len(o) > 0 {
		if _, err = b.TableContext(c, tx, OBJ_TBL).ReplaceInto(&o); err != nil {
			return fmt.Errorf("%w: PutObjAndData ReplaceInto OBJ failed (bktID=%d, objCount=%d): %v", ERR_EXEC_DB, bktID, len(o), err)
		}
	}

	// Commit transaction
	if err = tx.Commit(); err != nil {
		return fmt.Errorf("%w: PutObjAndData Commit transaction failed (bktID=%d): %v", ERR_EXEC_DB, bktID, err)
	}

	return nil
}

func (dma *DefaultMetadataAdapter) GetObj(c Ctx, bktID int64, ids []int64) (o []*ObjectInfo, err error) {
	// Use read connection for object retrieval
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, fmt.Errorf("%w: GetObj failed (bktID=%d, idsCount=%d): %v", ERR_QUERY_DB, bktID, len(ids), err)
	}
	return
}

func (dma *DefaultMetadataAdapter) SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error {
	// Use write connection for update operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(o, b.Fields(fields...), b.Where(b.Eq("id", o.ID))); err != nil {
		// 如果存在同名文件，会报错：Error: stepping, UNIQUE constraint failed: obj.name (19)
		if strings.Contains(err.Error(), "UNIQUE constraint failed") {
			return ERR_DUP_KEY
		}
		return fmt.Errorf("%w: SetObj failed (bktID=%d, objID=%d, fields=%v): %v", ERR_EXEC_DB, bktID, o.ID, fields, err)
	}
	return nil
}

func toDelim(field string, o *ObjectInfo) string {
	var d interface{}
	switch field {
	case "id":
		return fmt.Sprint(o.ID)
	case "name":
		return fmt.Sprint(o.Name)
	case "mtime":
		d = o.MTime
	case "size":
		d = o.Size
	case "type":
		d = o.Type
	}
	return fmt.Sprintf("%v:%d", d, o.ID)
}

// escapeLikePattern escapes special characters in a LIKE pattern for SQLite
// Converts wildcards: * -> %, ? -> _
// Escapes SQL special characters: ' -> ”
// This prevents SQL injection and ensures proper pattern matching
// 只遍历字符串一次，提高性能
func escapeLikePattern(pattern string) string {
	var builder strings.Builder
	changed := false

	// 只遍历一次字符串，同时处理通配符转换和SQL转义
	for i, r := range pattern {
		switch r {
		case '*':
			if !changed {
				// 第一次遇到需要处理的字符，初始化builder并写入之前的内容
				// 使用保守的容量预估：原长度的1.5倍（考虑单引号可能翻倍）
				builder.Grow(len(pattern) + len(pattern)/2)
				builder.WriteString(pattern[:i])
				changed = true
			}
			builder.WriteRune('%')
		case '?':
			if !changed {
				builder.Grow(len(pattern) + len(pattern)/2)
				builder.WriteString(pattern[:i])
				changed = true
			}
			builder.WriteRune('_')
		case '\'':
			if !changed {
				builder.Grow(len(pattern) + len(pattern)/2)
				builder.WriteString(pattern[:i])
				changed = true
			}
			builder.WriteString("''")
		default:
			if changed {
				builder.WriteRune(r)
			}
		}
	}

	if !changed {
		return pattern
	}
	return builder.String()
}

func doOrder(delim, order string, conds *[]interface{}) (string, string) {
	// 处理order
	if order == "" {
		order = "id"
	}
	fn := b.Gt
	orderBy := order
	switch order[0] {
	case '-':
		fn = b.Lt
		order = order[1:]
		orderBy = order + " desc"
	case '+':
		order = order[1:]
		orderBy = order
	}
	if order != "id" && order != "name" {
		orderBy = orderBy + ", id"
	}

	// 处理边界条件
	ds := strings.Split(delim, ":")
	if len(ds) > 0 && ds[0] != "" {
		if order == "id" || order == "name" {
			*conds = append(*conds, fn(order, ds[0]))
		} else if len(ds) == 2 {
			*conds = append(*conds, b.Or(fn(order, ds[0]),
				b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
		}
	}
	return orderBy, order
}

func (dma *DefaultMetadataAdapter) ListObj(c Ctx, bktID, pid int64,
	wd, delim, order string, count int) (o []*ObjectInfo,
	cnt int64, d string, err error,
) {
	conds := []interface{}{b.Eq("pid", pid)}
	if wd != "" {
		if strings.ContainsAny(wd, "*?") {
			// sqlite 分支使用 LIKE 模式匹配
			// 使用转义方法处理通配符和特殊字符
			pattern := escapeLikePattern(wd)
			// escapeLikePattern already handles single quotes (' -> ''), but we need to escape backslash
			// Escape backslash for SQL string literal (SQLite uses backslash for escaping in LIKE)
			escapedPattern := strings.ReplaceAll(pattern, "\\", "\\\\")
			// Use b.Cond with the escaped pattern as a parameter to avoid borm parsing issues
			// Note: ? has been converted to _ by escapeLikePattern, so no need to escape ? here
			conds = append(conds, b.Cond("name LIKE ?", escapedPattern))
		} else {
			conds = append(conds, b.Eq("name", wd))
		}
	}

	// Use read connection for listing objects
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, "", fmt.Errorf("%w: ListObj GetReadDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	// Count query should use conditions without pagination (delim)
	// So we need to do count query before adding pagination conditions
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", fmt.Errorf("%w: ListObj count failed (bktID=%d, pid=%d, wd=%s): %v", ERR_QUERY_DB, bktID, pid, wd, err)
	}

	// Build order conditions and add pagination conditions for data query
	var orderBy string
	orderBy, order = doOrder(delim, order, &conds)

	// Only query data if count > 0
	if count > 0 {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(count)); err != nil {
			return nil, 0, "", fmt.Errorf("%w: ListObj select failed (bktID=%d, pid=%d, wd=%s, count=%d): %v", ERR_QUERY_DB, bktID, pid, wd, count, err)
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}

func (dba *DefaultBaseMetadataAdapter) PutUsr(c Ctx, u *UserInfo) error {
	// Use write connection for user creation/update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).ReplaceInto(&u); err != nil {
		return fmt.Errorf("%w: PutUsr failed (userID=%d, usr=%s): %v", ERR_EXEC_DB, u.ID, u.Usr, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) GetUsr(c Ctx, ids []int64) (o []*UserInfo, err error) {
	// Use read connection for user retrieval
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, fmt.Errorf("%w: GetUsr failed (idsCount=%d): %v", ERR_QUERY_DB, len(ids), err)
	}
	return
}

func (dba *DefaultBaseMetadataAdapter) GetUsr2(c Ctx, usr string) (o *UserInfo, err error) {
	// Use read connection for user lookup
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	o = &UserInfo{}
	if _, err = b.TableContext(c, db, USR_TBL).Select(o, b.Where(b.Eq("usr", usr))); err != nil {
		return nil, fmt.Errorf("%w: GetUsr2 failed (usr=%s): %v", ERR_QUERY_DB, usr, err)
	}
	return
}

func (dba *DefaultBaseMetadataAdapter) SetUsr(c Ctx, fields []string, u *UserInfo) error {
	// Use write connection for user update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Update(&u,
		b.Fields(fields...), b.Where(b.Eq("id", u.ID))); err != nil {
		return fmt.Errorf("%w: SetUsr failed (userID=%d, fields=%v): %v", ERR_EXEC_DB, u.ID, fields, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) ListUsers(c Ctx) (o []*UserInfo, err error) {
	// Use read connection for listing users
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Use borm to query all users
	if _, err = b.TableContext(c, db, USR_TBL).Select(&o); err != nil {
		return nil, fmt.Errorf("%w: ListUsers failed: %v", ERR_QUERY_DB, err)
	}

	// 清除敏感信息
	for i := range o {
		o[i].Pwd = ""
	}
	return
}

func (dba *DefaultBaseMetadataAdapter) DeleteUser(c Ctx, userID int64) error {
	// Use write connection for user deletion
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Delete(b.Where(b.Eq("id", userID))); err != nil {
		return fmt.Errorf("%w: DeleteUser failed (userID=%d): %v", ERR_EXEC_DB, userID, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) PutBkt(c Ctx, o []*BucketInfo) error {
	// Bucket info is now stored in bucket database, not main database
	// CmprWay is now smart compression by default (checks file type)
	uid := getUID(c) // Get UID from context for ACL creation

	for _, bucket := range o {
		if bucket != nil {
			if bucket.ChunkSize <= 0 {
				bucket.ChunkSize = DEFAULT_CHUNK_SIZE
			}
		}
	}

	// Initialize bucket database and write bucket info to it
	for _, x := range o {
		if err := InitBucketDB(c, x.ID); err != nil {
			return fmt.Errorf("%w: PutBkt InitBucketDB failed (bktID=%d): %v", ERR_EXEC_DB, x.ID, err)
		}

		// Use write connection for bucket database
		db, err := GetWriteDB(c, x.ID)
		if err != nil {
			return fmt.Errorf("%w: PutBkt GetWriteDB failed (bktID=%d): %v", ERR_OPEN_DB, x.ID, err)
		}
		// Note: Don't close the connection, it's from the pool

		// Write bucket info to bucket database
		bktSlice := []*BucketInfo{x}
		if _, err = b.TableContext(c, db, BKT_TBL).ReplaceInto(&bktSlice); err != nil {
			return fmt.Errorf("%w: PutBkt failed (bktID=%d, count=%d): %v", ERR_EXEC_DB, x.ID, len(o), err)
		}

		// Create ACL for the bucket (if uid is available)
		if uid > 0 {
			if err = dba.PutACL(c, x.ID, uid, ALL); err != nil {
				return fmt.Errorf("%w: PutBkt PutACL failed (bktID=%d, uid=%d): %v", ERR_EXEC_DB, x.ID, uid, err)
			}
		}
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) GetBkt(c Ctx, ids []int64) (o []*BucketInfo, err error) {
	// Bucket info is now stored in bucket database, not main database
	// Read from each bucket's database in parallel
	if len(ids) == 0 {
		return []*BucketInfo{}, nil
	}

	var wg sync.WaitGroup
	var mu sync.Mutex
	results := make([]*BucketInfo, 0, len(ids))

	for _, bktID := range ids {
		wg.Add(1)
		go func(id int64) {
			defer wg.Done()

			// Use read connection for bucket database
			db, err := GetReadDB(c, id)
			if err != nil {
				// If bucket database doesn't exist, skip it
				return
			}
			// Note: Don't close the connection, it's from the pool

			var bucketInfo []*BucketInfo
			if _, err = b.TableContext(c, db, BKT_TBL).Select(&bucketInfo, b.Where(b.Eq("id", id))); err != nil {
				// If query fails, skip this bucket
				return
			}
			if len(bucketInfo) > 0 {
				mu.Lock()
				results = append(results, bucketInfo[0])
				mu.Unlock()
			}
		}(bktID)
	}

	wg.Wait()
	// Sort results by ID to ensure consistent ordering
	sort.Slice(results, func(i, j int) bool {
		return results[i].ID < results[j].ID
	})
	return results, nil
}

func (dba *DefaultBaseMetadataAdapter) ListBkt(c Ctx, uid int64) (o []*BucketInfo, err error) {
	// Bucket info is now stored in bucket database, not main database
	// Use ACL to find buckets owned by this user
	acls, err := dba.ListACLByUser(c, uid)
	if err != nil {
		return nil, err
	}
	if len(acls) == 0 {
		return []*BucketInfo{}, nil
	}

	// Get bucket IDs from ACLs
	bktIDs := make([]int64, 0, len(acls))
	for _, acl := range acls {
		bktIDs = append(bktIDs, acl.BktID)
	}

	// Get bucket info for these buckets
	return dba.GetBkt(c, bktIDs)
}

func (dba *DefaultBaseMetadataAdapter) ListAllBuckets(c Ctx) (o []*BucketInfo, err error) {
	// Bucket info is now stored in bucket database, not main database
	// Scan bucket databases to find all buckets in parallel
	dataDir := getDataPath(c)
	entries, err := os.ReadDir(dataDir)
	if err != nil {
		// If data directory doesn't exist, return empty list
		return []*BucketInfo{}, nil
	}

	var wg sync.WaitGroup
	var mu sync.Mutex
	results := make([]*BucketInfo, 0)

	for _, entry := range entries {
		if !entry.IsDir() {
			continue
		}
		bktID, err := strconv.ParseInt(entry.Name(), 10, 64)
		if err != nil {
			continue
		}

		wg.Add(1)
		go func(id int64) {
			defer wg.Done()

			// Try to read bucket info from bucket database
			db, err := GetReadDB(c, id)
			if err != nil {
				return
			}
			// Note: Don't close the connection, it's from the pool

			var bucketInfo []*BucketInfo
			if _, err = b.TableContext(c, db, BKT_TBL).Select(&bucketInfo); err != nil {
				return
			}
			if len(bucketInfo) > 0 {
				mu.Lock()
				results = append(results, bucketInfo...)
				mu.Unlock()
			}
		}(bktID)
	}

	wg.Wait()
	// Sort results by ID to ensure consistent ordering
	sort.Slice(results, func(i, j int) bool {
		return results[i].ID < results[j].ID
	})
	return results, nil
}

func (dba *DefaultBaseMetadataAdapter) DeleteBkt(c Ctx, bktID int64) error {
	// Bucket info is now stored in bucket database, not main database
	// Use write connection for bucket database
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return fmt.Errorf("%w: DeleteBkt GetWriteDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Delete(b.Where(b.Eq("id", bktID))); err != nil {
		return fmt.Errorf("%w: DeleteBkt failed (bktID=%d): %v", ERR_EXEC_DB, bktID, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) UpdateBktQuota(c Ctx, bktID int64, quota int64) error {
	// Bucket info is now stored in bucket database, not main database
	// Use write connection for bucket database
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return fmt.Errorf("%w: UpdateBktQuota GetWriteDB failed (bktID=%d): %v", ERR_OPEN_DB, bktID, err)
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Update(&BucketInfo{Quota: quota},
		b.Fields("quota"), b.Where(b.Eq("id", bktID))); err != nil {
		return fmt.Errorf("%w: UpdateBktQuota failed (bktID=%d, quota=%d): %v", ERR_EXEC_DB, bktID, quota, err)
	}
	return nil
}

func (dba *DefaultBaseMetadataAdapter) IncBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, size, 0, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) DecBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, -size, 0, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) IncBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, size, 0, 0, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) DecBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, -size, 0, 0, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) IncBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, size, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) DecBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, -size, 0)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) IncBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, size)
	return nil
}

func (dba *DefaultBaseMetadataAdapter) DecBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, -size)
	return nil
}

func (dma *DefaultMetadataAdapter) CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error) {
	// Use read connection for counting references
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if len(dataIDs) == 0 {
		return make(map[int64]int64), nil
	}

	// 统计每个DataID被引用的次数
	type refCount struct {
		DataID int64 `borm:"did"`
		Count  int64 `borm:"count(1)"`
	}
	var counts []refCount
	// 排除已删除的对象（pid >= 0，因为删除时PID会被翻转成负数）
	whereConds := []interface{}{b.In("did", dataIDs), "pid >= 0"}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&counts,
		b.Fields("did", "count(1)"),
		b.Where(whereConds...),
		b.GroupBy("did")); err != nil {
		return nil, ERR_QUERY_DB
	}

	result := make(map[int64]int64)
	for _, c := range counts {
		result[c.DataID] = c.Count
	}
	// 确保所有查询的dataID都在结果中（即使引用数为0）
	for _, dataID := range dataIDs {
		if _, exists := result[dataID]; !exists {
			result[dataID] = 0
		}
	}
	return result, nil
}

func (dma *DefaultMetadataAdapter) ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error) {
	// Use read connection for listing objects by type
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	var cnt int64
	// List objects by type, excluding deleted objects (pid >= 0)
	// Use b.And to combine conditions like in other places
	conds := []interface{}{b.Eq("t", objType), b.Gte("pid", 0)}
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...))
	if err != nil {
		// Log detailed error for debugging
		log.Printf("[ListObjsByType] Query failed: bktID=%d, objType=%d, error=%v", bktID, objType, err)
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var objs []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			// Log detailed error for debugging
			log.Printf("[ListObjsByType] Query failed: bktID=%d, objType=%d, offset=%d, limit=%d, error=%v", bktID, objType, offset, limit, err)
			return nil, 0, ERR_QUERY_DB
		}
	}
	return objs, cnt, nil
}

func (dma *DefaultMetadataAdapter) ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error) {
	// Use read connection for listing children
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	var cnt int64
	// List children of specified parent (pid)
	// Note: We don't need "pid >= 0" here because we're querying children of a specific parent
	// If the parent is deleted (pid < 0), we shouldn't be querying its children anyway
	conds := []interface{}{b.Eq("pid", pid)}
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...))
	if err != nil {
		// Log detailed error for debugging
		log.Printf("[ListChildren] Query failed: bktID=%d, pid=%d, error=%v", bktID, pid, err)
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var children []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&children,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			// Log detailed error for debugging
			log.Printf("[ListChildren] Query failed: bktID=%d, pid=%d, offset=%d, limit=%d, error=%v", bktID, pid, offset, limit, err)
			return nil, 0, ERR_QUERY_DB
		}
	}
	return children, cnt, nil
}

func (dma *DefaultMetadataAdapter) GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error) {
	// Use read connection for querying objects by DataID
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	var objs []*ObjectInfo
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
		b.Where(b.Eq("did", dataID)))
	if err != nil {
		return nil, fmt.Errorf("%w: GetObjByDataID failed (bktID=%d, dataID=%d): %v", ERR_QUERY_DB, bktID, dataID, err)
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error) {
	// Use read connection for listing versions
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_QUERY_DB
	}
	// Note: Don't close the connection, it's from the pool

	var versions []*ObjectInfo
	// Query all version objects (type=3, pid=fileID)
	// If excludeWriting is true, exclude writing versions (name="0")
	conds := []interface{}{b.Eq("pid", fileID), b.Eq("t", OBJ_TYPE_VERSION)}
	if excludeWriting {
		conds = append(conds, b.Neq("name", WritingVersionName))
	}
	// Use "mtime desc" format (lowercase) as borm expects
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&versions,
		b.Where(conds...),
		b.OrderBy("mtime desc"))
	if err != nil {
		// Return ERR_QUERY_DB with detailed error information
		return nil, fmt.Errorf("%w: ListVersions failed (bktID=%d, fileID=%d, excludeWriting=%v): %v", ERR_QUERY_DB, bktID, fileID, excludeWriting, err)
	}
	return versions, nil
}

func (dma *DefaultMetadataAdapter) DeleteObj(c Ctx, bktID int64, id int64) error {
	// Use write connection for delete operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get object information (read operation, but using same connection for consistency)
	obj := &ObjectInfo{}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(obj, b.Where(b.Eq("id", id))); err != nil {
		return fmt.Errorf("%w: GetObjByDataID select obj failed (bktID=%d, id=%d): %v", ERR_QUERY_DB, bktID, id, err)
	}

	// Flip object's PID to negative to mark as deleted (so it disappears from original tree structure)
	// Also update MTime to current timestamp
	// Note: If original PID is 0 (ROOT_OID), use -1 as special marker
	newPID := -obj.PID
	if newPID == 0 {
		newPID = -1 // Negative of ROOT_OID is still 0, use -1 as special marker
	}
	currentTime := Now()

	// Check if there's already a deleted object with same name in same parent directory (PID < 0 and |PID| == original PID)
	// If exists, rename current object to avoid conflict
	var conflictingObjs []*ObjectInfo
	conflictCond := []interface{}{b.Eq("pid", newPID), b.Eq("name", obj.Name), b.Neq("id", id)}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&conflictingObjs,
		b.Where(conflictCond...)); err == nil && len(conflictingObjs) > 0 {
		// Conflict exists, rename current object (add timestamp suffix)
		ext := filepath.Ext(obj.Name)
		nameWithoutExt := strings.TrimSuffix(obj.Name, ext)
		newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, currentTime, ext)
		obj.Name = newName
	}

	// Update object's PID and MTime, also update name if there's a conflict
	updateFields := []string{"pid", "mtime"}
	hasNameChange := len(conflictingObjs) > 0
	if hasNameChange {
		updateFields = append(updateFields, "name")
	}

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(&ObjectInfo{
		ID:    id,
		PID:   newPID,
		MTime: currentTime,
		Name:  obj.Name,
	}, b.Fields(updateFields...), b.Where(b.Eq("id", id))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	var objs []*ObjectInfo
	// PID < 0 or PID = -1 indicates deleted objects
	conds := []interface{}{"pid < 0 OR pid = -1"}
	if beforeTime > 0 {
		conds = append(conds, b.Lte("mtime", beforeTime))
	}

	if limit > 0 {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime"),
			b.Limit(limit)); err != nil {
			return nil, fmt.Errorf("%w: ListRecycleBin select failed (bktID=%d, limit=%d): %v", ERR_QUERY_DB, bktID, limit, err)
		}
	} else {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime")); err != nil {
			return nil, fmt.Errorf("%w: ListRecycleBin select all failed (bktID=%d): %v", ERR_QUERY_DB, bktID, err)
		}
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListRecycleBin(c Ctx, bktID int64, opt ListOptions) (o []*ObjectInfo, cnt int64, d string, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, "", ERR_OPEN_DB
	}
	defer db.Close()

	// PID < 0 或 PID = -1 表示已删除的对象
	conds := []interface{}{"pid < 0 OR pid = -1"}

	// Process filter word
	if opt.Word != "" {
		if strings.ContainsAny(opt.Word, "*?") {
			// SQLite branch uses LIKE pattern matching
			// 使用转义方法处理通配符和特殊字符
			pattern := escapeLikePattern(opt.Word)
			// escapeLikePattern already handles single quotes (' -> ''), but we need to escape backslash
			// Escape backslash for SQL string literal (SQLite uses backslash for escaping in LIKE)
			escapedPattern := strings.ReplaceAll(pattern, "\\", "\\\\")
			// Use b.Cond with the escaped pattern as a parameter to avoid borm parsing issues
			conds = append(conds, b.Cond("name LIKE ?", escapedPattern))
		} else {
			conds = append(conds, b.Eq("name", opt.Word))
		}
	}

	// Get total count
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", fmt.Errorf("%w: ListRecycleBin count failed (bktID=%d, word=%s): %v", ERR_QUERY_DB, bktID, opt.Word, err)
	}

	if opt.Count > 0 {
		// Process sorting
		order := opt.Order
		if order == "" {
			order = "mtime"
		}
		fn := b.Gt
		orderBy := order
		switch order[0] {
		case '-':
			fn = b.Lt
			order = order[1:]
			orderBy = order + " desc"
		case '+':
			order = order[1:]
			orderBy = order
		}
		if order != "id" && order != "name" {
			orderBy = orderBy + ", id"
		}

		// Process delimiter (for pagination)
		if opt.Delim != "" {
			ds := strings.Split(opt.Delim, ":")
			if len(ds) > 0 && ds[0] != "" {
				if order == "id" || order == "name" {
					conds = append(conds, fn(order, ds[0]))
				} else if len(ds) == 2 {
					conds = append(conds, b.Or(fn(order, ds[0]),
						b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
				}
			}
		}

		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(opt.Count)); err != nil {
			return nil, 0, "", fmt.Errorf("%w: ListRecycleBin select failed (bktID=%d, word=%s, count=%d): %v", ERR_QUERY_DB, bktID, opt.Word, opt.Count, err)
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}
