package core

import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	_ "github.com/mattn/go-sqlite3"
	b "github.com/orca-zhang/borm"
)

type BucketInfo struct {
	ID           int64  `borm:"id" json:"i,omitempty"`             // Bucket ID
	Name         string `borm:"name" json:"n,omitempty"`           // Bucket name
	UID          int64  `borm:"uid" json:"u,omitempty"`            // Owner
	Type         int    `borm:"type" json:"t,omitempty"`           // Bucket type, 0: none, 1: normal ...
	Quota        int64  `borm:"quota" json:"q,omitempty"`          // Quota, negative means unlimited
	Used         int64  `borm:"used" json:"s,omitempty"`           // Logical usage, counts original size of all versions
	RealUsed     int64  `borm:"real_used" json:"ru,omitempty"`     // Actual physical usage, counts actual stored data size
	LogicalUsed  int64  `borm:"logical_used" json:"lu,omitempty"`  // Logical occupancy, counts logical size of all valid objects (not deleted, PID >= 0) considering deduplication but excluding deleted objects
	DedupSavings int64  `borm:"dedup_savings" json:"ds,omitempty"` // Instant upload space savings, counts deduplicated data savings (LogicalUsed - unique data block size)
	ChunkSize    int64  `borm:"chunk_size" json:"cs,omitempty"`    // Chunk size (bytes), must be >0 (defaults to system chunk size)
	Key          string `borm:"key" json:"k,omitempty"`            // Database encryption key
	CmprWay      uint32 `borm:"cmpr_way" json:"cw,omitempty"`      // Compression method (DATA_CMPR_MASK), default is smart compression (checks file type)
	CmprQlty     uint32 `borm:"cmpr_qlty" json:"cq,omitempty"`     // Compression quality
	EndecWay     uint32 `borm:"endec_way" json:"ew,omitempty"`     // Encryption method (DATA_ENDEC_MASK)
	EndecKey     string `borm:"endec_key" json:"ek,omitempty"`     // Encryption key
	RefLevel     uint32 `borm:"ref_level" json:"rl,omitempty"`     // Instant upload level: 0=OFF, 1=FULL, 2=FAST
	// SnapshotID int64 // Latest snapshot version ID
}

type UserInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`     // User ID
	Usr    string `borm:"usr" json:"u,omitempty"`    // Username
	Pwd    string `borm:"pwd" json:"p,omitempty"`    // Password, encrypted using PBKDF2-HMAC-SHA256
	Role   uint32 `borm:"role" json:"r,omitempty"`   // User role: regular user / administrator
	Name   string `borm:"name" json:"n,omitempty"`   // Name
	Avatar string `borm:"avatar" json:"a,omitempty"` // Avatar
}

// Object types
const (
	OBJ_TYPE_MALFORMED = iota - 1
	OBJ_TYPE_NONE
	OBJ_TYPE_DIR
	OBJ_TYPE_FILE
	OBJ_TYPE_VERSION
	OBJ_TYPE_PREVIEW
)

type ObjectInfo struct {
	ID     int64  `borm:"id" json:"i,omitempty"`    // Object ID (randomly generated by idgen)
	PID    int64  `borm:"pid" json:"p,omitempty"`   // Parent object ID
	MTime  int64  `borm:"mtime" json:"m,omitempty"` // Update time, second-level timestamp
	DataID int64  `borm:"did" json:"d,omitempty"`   // Data ID, if 0, no data (newly created file, DataID is object ID, serving as first version data)
	Type   int    `borm:"type" json:"t,omitempty"`  // Object type, -1: malformed, 0: none, 1: dir, 2: file, 3: version, 4: preview(thumb/m3u8/pdf)
	Name   string `borm:"name" json:"n,omitempty"`  // Object name
	Size   int64  `borm:"size" json:"s,omitempty"`  // Object size, directory size is child object count, file size is latest version byte count
	Extra  string `borm:"ext" json:"e,omitempty"`   // Object extended information
}

// Data status
const (
	DATA_NORMAL         = uint32(1 << iota) // Normal
	DATA_ENDEC_AES256                       // Whether AES encrypted
	DATA_ENDEC_SM4                          // Whether SM4 encrypted
	DATA_ENDEC_RESERVED                     // Whether reserved encryption
	DATA_CMPR_SNAPPY                        // Whether snappy compressed
	DATA_CMPR_ZSTD                          // Whether zstd compressed
	DATA_CMPR_GZIP                          // Whether gzip compressed
	DATA_CMPR_BR                            // Whether brotli compressed
	DATA_KIND_IMG                           // Image type
	DATA_KIND_VIDEO                         // Video type
	DATA_KIND_AUDIO                         // Audio type
	DATA_KIND_ARCHIVE                       // Archive type
	DATA_KIND_DOCS                          // Document type
	DATA_KIND_FONT                          // Font type
	DATA_KIND_APP                           // Application type
	DATA_KIND_RESERVED                      // Unknown type
	DATA_SPARSE                             // Sparse file: chunks may not exist, read as zeros

	DATA_MALFORMED  = 0 // Whether corrupted
	DATA_ENDEC_MASK = DATA_ENDEC_AES256 | DATA_ENDEC_SM4 | DATA_ENDEC_RESERVED
	DATA_CMPR_MASK  = DATA_CMPR_SNAPPY | DATA_CMPR_ZSTD | DATA_CMPR_GZIP | DATA_CMPR_BR
	DATA_KIND_MASK  = DATA_KIND_IMG | DATA_KIND_VIDEO | DATA_KIND_AUDIO | DATA_KIND_ARCHIVE | DATA_KIND_DOCS | DATA_KIND_FONT | DATA_KIND_APP | DATA_KIND_RESERVED
)

type DataInfo struct {
	ID        int64  `borm:"id" json:"i,omitempty"`      // Data ID (randomly generated by idgen)
	Size      int64  `borm:"size" json:"s,omitempty"`    // Data size (compressed/encrypted)
	OrigSize  int64  `borm:"o_size" json:"r,omitempty"`  // Original data size (before compression/encryption)
	HdrCRC32  uint32 `borm:"h_crc32" json:"h,omitempty"` // CRC32 checksum of first 100KB
	CRC32     uint32 `borm:"crc32" json:"c,omitempty"`   // CRC32 checksum of entire original data
	MD5       int64  `borm:"md5" json:"m,omitempty"`     // MD5 hash of entire original data
	Cksum     uint32 `borm:"cksum" json:"u,omitempty"`   // CRC32 checksum of final data (for consistency audit)
	Kind      uint32 `borm:"kind" json:"k,omitempty"`    // Data status: normal, corrupted, encrypted, compressed, type (for preview, etc.)
	PkgID     int64  `borm:"pkg_id" json:"p,omitempty"`  // Package data ID (also generated by idgen)
	PkgOffset uint32 `borm:"pkg_off" json:"g,omitempty"` // Offset position in package data
	// PkgID != 0 indicates packaged data
	// SnapshotID int64 // Snapshot version ID
}

const EmptyDataID = 4708888888888

// WritingVersionName is the special name for "writing version" that allows direct data block modification
// Versions with name="0" can be directly modified without creating new versions
const WritingVersionName = "0"

func EmptyDataInfo() *DataInfo {
	return &DataInfo{
		ID:   EmptyDataID,
		MD5:  -1081059644736014743,
		Kind: DATA_NORMAL,
	}
}

// IsSparseFile checks if DataInfo represents a sparse file
// Sparse files have DATA_SPARSE flag set, meaning chunks may not exist and should be read as zeros
func IsSparseFile(dataInfo *DataInfo) bool {
	if dataInfo == nil {
		return false
	}
	return dataInfo.Kind&DATA_SPARSE != 0
}

// MarkSparseFile marks a DataInfo as sparse file
func MarkSparseFile(dataInfo *DataInfo) {
	if dataInfo != nil {
		dataInfo.Kind |= DATA_SPARSE
	}
}

const (
	BKT_TBL = "bkt"
	USR_TBL = "usr"

	OBJ_TBL  = "obj"
	DATA_TBL = "data"
)

type UserMetadataAdapter interface {
	PutUsr(c Ctx, u *UserInfo) error
	GetUsr(c Ctx, ids []int64) ([]*UserInfo, error)
	GetUsr2(c Ctx, usr string) (*UserInfo, error)
	SetUsr(c Ctx, fields []string, u *UserInfo) error
	ListUsers(c Ctx) ([]*UserInfo, error)
	DeleteUser(c Ctx, userID int64) error
}

type BucketMetadataAdapter interface {
	PutBkt(c Ctx, o []*BucketInfo) error
	DeleteBkt(c Ctx, bktID int64) error
	GetBkt(c Ctx, ids []int64) ([]*BucketInfo, error)
	ListBkt(c Ctx, uid int64) ([]*BucketInfo, error)
	ListAllBuckets(c Ctx) ([]*BucketInfo, error) // Get all buckets (for scheduled tasks)
	// Update bucket quota and usage
	UpdateBktQuota(c Ctx, bktID int64, quota int64) error
	// Increase bucket's actual usage (when uploading data)
	IncBktRealUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's actual usage (when deleting data)
	DecBktRealUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical usage (when creating objects, including instant upload)
	IncBktUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical usage (when deleting objects)
	DecBktUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's logical occupancy (when creating objects, only count valid objects)
	IncBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Decrease bucket's logical occupancy (when deleting objects)
	DecBktLogicalUsed(c Ctx, bktID int64, size int64) error
	// Increase bucket's deduplication savings (saved data size from instant upload)
	IncBktDedupSavings(c Ctx, bktID int64, size int64) error
	// Decrease bucket's deduplication savings (when deleting objects)
	DecBktDedupSavings(c Ctx, bktID int64, size int64) error
}

// DuplicateGroup represents a group of duplicate data with same checksums
type DuplicateGroup struct {
	Key     string  // Checksum key: "OrigSize:HdrCRC32:CRC32:MD5"
	DataIDs []int64 // List of DataIDs with same checksums
}

type DataMetadataAdapter interface {
	RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error)
	PutData(c Ctx, bktID int64, d []*DataInfo) error
	GetData(c Ctx, bktID, id int64) (*DataInfo, error)
	// PutDataAndObj writes both DataInfo and ObjectInfo in a single transaction
	// This optimization reduces database round trips
	PutDataAndObj(c Ctx, bktID int64, d []*DataInfo, o []*ObjectInfo) error
	ListAllData(c Ctx, bktID int64, offset, limit int) ([]*DataInfo, int64, error) // offset: offset, limit: page size, returns data and total count
	// Find duplicate data: returns DataID groups with same checksums (OrigSize, HdrCRC32, CRC32, MD5 all same)
	FindDuplicateData(c Ctx, bktID int64, offset, limit int) ([]DuplicateGroup, int64, error) // Returns duplicate data groups and total count
	// Update object's DataID reference
	UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error
	// Delete data metadata
	DeleteData(c Ctx, bktID int64, dataIDs []int64) error
	// Find small file data that can be packaged (for defragmentation)
	FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) ([]*DataInfo, int64, error)
}

type ObjectMetadataAdapter interface {
	PutObj(c Ctx, bktID int64, o []*ObjectInfo) ([]int64, error)
	GetObj(c Ctx, bktID int64, ids []int64) ([]*ObjectInfo, error)
	SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error
	ListObj(c Ctx, bktID, pid int64, wd, delim, order string, count int) ([]*ObjectInfo, int64, string, error)
	CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error)               // Count DataID references
	DeleteObj(c Ctx, bktID int64, id int64) error                                             // Delete object (mark as deleted, flip PID to negative)
	ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error)   // List deleted objects (PID < 0)
	ListRecycleBin(c Ctx, bktID int64, opt ListOptions) ([]*ObjectInfo, int64, string, error) // List recycle bin (objects with PID < 0)
	// Query all objects of specified type (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all child objects under specified directory (not deleted, pid >= 0)
	// offset: offset, limit: page size, returns data and total count
	ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error)
	// Query all objects that reference the specified DataID
	GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error)
	// Query all versions of a file (sorted by MTime descending, latest first)
	// excludeWriting: if true, exclude writing versions (name="0")
	ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error)
}

type MetadataAdapter interface {
	Close()

	BucketMetadataAdapter
	UserMetadataAdapter
	DataMetadataAdapter
	ObjectMetadataAdapter
}

func GetDB(c ...interface{}) (*sql.DB, error) {
	// Use proper parameters for concurrent access:
	// - _journal=WAL: Write-Ahead Logging for better concurrency
	// - cache=shared: Share cache between connections
	// - mode=rwc: Read-Write-Create mode
	// - _busy_timeout=10000: Wait up to 10 seconds for locks (increased for high concurrency)
	// - _txlock=immediate: Use immediate transaction locks to reduce contention
	param := "?_journal=WAL&cache=shared&mode=rwc&_busy_timeout=10000&_txlock=immediate"
	dirPath := ORCAS_BASE
	var dbKey string

	if len(c) > 0 {
		// Check if first parameter is a string (database key)
		if keyStr, ok := c[0].(string); ok {
			dbKey = keyStr
			c = c[1:] // Remove key from remaining params
		} else if ctx, ok := c[0].(Ctx); ok {
			// If it's a Ctx, try to get key from context
			if key := getKey(ctx); key != "" {
				dbKey = key
			}
			c = c[1:] // Remove context from remaining params
		} else if _, ok := c[0].(context.Context); ok {
			// Handle standard context.Context (shouldn't happen, but be safe)
			// Try to convert to Ctx and extract key
			if key := getKey(Ctx(c[0].(context.Context))); key != "" {
				dbKey = key
			}
			c = c[1:] // Remove context from remaining params
		}
	}

	if len(c) > 0 {
		// Next parameter should be bucket ID (int64)
		// Ensure it's actually an int64, not accidentally a context
		if bktID, ok := c[0].(int64); ok {
			dirPath = filepath.Join(ORCAS_DATA, fmt.Sprint(bktID))
		} else {
			// Invalid parameter type - this should not happen
			// Log error but try to continue with default path
			return nil, fmt.Errorf("invalid bucket ID type: expected int64, got %T", c[0])
		}
		if len(c) > 1 {
			if ctx, ok := c[1].(Ctx); ok {
				if key := getKey(ctx); key != "" {
					dbKey = key
				}
			}
		}
	}

	if dbKey != "" {
		param += "&key=" + dbKey
	}

	os.MkdirAll(dirPath, 0o766)
	db, err := sql.Open("sqlite3", filepath.Join(dirPath, "meta.db")+param)
	if err != nil {
		return nil, err
	}

	// Set connection pool limits for better concurrency
	// MaxOpenConns = 25: Allow up to 25 concurrent connections (good for 10 concurrent clients)
	// MaxIdleConns = 10: Keep 10 connections in pool for reuse
	// ConnMaxLifetime = 0: Connections don't expire (reuse indefinitely)
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(0)

	return db, nil
}

// GetDBWithKey opens database with specified encryption key
func GetDBWithKey(key string) (*sql.DB, error) {
	param := "?_journal=WAL&cache=shared&mode=rwc&_busy_timeout=10000&_txlock=immediate"
	if key != "" {
		param += "&key=" + key
	}
	dirPath := ORCAS_BASE
	os.MkdirAll(dirPath, 0o766)
	db, err := sql.Open("sqlite3", filepath.Join(dirPath, "meta.db")+param)
	if err != nil {
		return nil, err
	}

	// Set connection pool limits
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(0)

	return db, nil
}

// ChangeDBKey changes the database encryption key
// This function re-encrypts the database with a new key by:
// 1. Exporting all data from old database
// 2. Creating a new database with new key
// 3. Importing all data into new database
// 4. Replacing old database with new one
func ChangeDBKey(oldKey, newKey string) error {
	// If new key is same as old key, no change needed
	if oldKey == newKey {
		return nil
	}

	// Get database file path
	dbPath := filepath.Join(ORCAS_BASE, "meta.db")

	// Check if database file exists before opening
	if _, err := os.Stat(dbPath); os.IsNotExist(err) {
		return fmt.Errorf("database file does not exist")
	}

	// Open database with old key
	dbOld, err := GetDBWithKey(oldKey)
	if err != nil {
		return ERR_OPEN_DB
	}
	defer dbOld.Close()

	// Verify old key works by querying
	var count int
	err = dbOld.QueryRow("SELECT COUNT(*) FROM sqlite_master WHERE type='table'").Scan(&count)
	if err != nil {
		return fmt.Errorf("old key verification failed: %v", err)
	}

	tempPath := dbPath + ".tmp"

	// Step 1: Create temporary database with new key
	dbNew, err := func() (*sql.DB, error) {
		param := "?_journal=WAL&cache=shared&mode=rwc&nolock=1"
		if newKey != "" {
			param += "&key=" + newKey
		}
		return sql.Open("sqlite3", tempPath+param)
	}()
	if err != nil {
		return fmt.Errorf("failed to create new database: %v", err)
	}
	defer dbNew.Close()

	// Step 2: Get all table names
	rows, err := dbOld.Query("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
	if err != nil {
		return fmt.Errorf("failed to query tables: %v", err)
	}
	defer rows.Close()

	var tables []string
	for rows.Next() {
		var tableName string
		if err := rows.Scan(&tableName); err != nil {
			continue
		}
		tables = append(tables, tableName)
	}
	rows.Close()

	// Step 3: Create all tables in new database
	for _, tableName := range tables {
		// Get CREATE TABLE statement
		var createSQL string
		err = dbOld.QueryRow("SELECT sql FROM sqlite_master WHERE type='table' AND name=?", tableName).Scan(&createSQL)
		if err != nil {
			return fmt.Errorf("failed to get CREATE TABLE for %s: %v", tableName, err)
		}

		// Execute CREATE TABLE in new database
		_, err = dbNew.Exec(createSQL)
		if err != nil {
			return fmt.Errorf("failed to create table %s: %v", tableName, err)
		}
	}

	// Step 4: Copy all data from old to new database
	for _, tableName := range tables {
		// Get all data from old table
		rows, err := dbOld.Query(fmt.Sprintf("SELECT * FROM %s", tableName))
		if err != nil {
			return fmt.Errorf("failed to query table %s: %v", tableName, err)
		}

		// Get column names
		columns, err := rows.Columns()
		if err != nil {
			rows.Close()
			return fmt.Errorf("failed to get columns for %s: %v", tableName, err)
		}

		// Build INSERT statement
		placeholders := strings.Repeat("?,", len(columns))
		placeholders = placeholders[:len(placeholders)-1] // Remove trailing comma
		insertSQL := fmt.Sprintf("INSERT INTO %s (%s) VALUES (%s)",
			tableName, strings.Join(columns, ","), placeholders)

		// Insert rows
		for rows.Next() {
			values := make([]interface{}, len(columns))
			valuePtrs := make([]interface{}, len(columns))
			for i := range values {
				valuePtrs[i] = &values[i]
			}

			if err := rows.Scan(valuePtrs...); err != nil {
				rows.Close()
				return fmt.Errorf("failed to scan row from %s: %v", tableName, err)
			}

			_, err = dbNew.Exec(insertSQL, values...)
			if err != nil {
				rows.Close()
				return fmt.Errorf("failed to insert row into %s: %v", tableName, err)
			}
		}
		rows.Close()
	}

	// Step 5: Copy indexes
	indexRows, err := dbOld.Query("SELECT sql FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%'")
	if err == nil {
		defer indexRows.Close()
		for indexRows.Next() {
			var indexSQL string
			if err := indexRows.Scan(&indexSQL); err != nil {
				continue
			}
			if indexSQL != "" {
				_, _ = dbNew.Exec(indexSQL) // Ignore errors for indexes
			}
		}
	}

	// Step 6: Close databases
	dbOld.Close()
	dbNew.Close()

	// Step 7: Replace old database with new one
	// Backup old database first
	backupPath := dbPath + ".backup"
	if err := os.Rename(dbPath, backupPath); err != nil {
		return fmt.Errorf("failed to backup old database: %v", err)
	}

	// Move new database to final location
	if err := os.Rename(tempPath, dbPath); err != nil {
		// Restore backup on error
		os.Rename(backupPath, dbPath)
		return fmt.Errorf("failed to replace database: %v", err)
	}

	// Step 8: Verify new key works
	dbVerify, err := GetDBWithKey(newKey)
	if err != nil {
		// Restore backup on error
		os.Rename(backupPath, dbPath)
		return ERR_OPEN_DB
	}
	defer dbVerify.Close()

	err = dbVerify.QueryRow("SELECT COUNT(*) FROM sqlite_master WHERE type='table'").Scan(&count)
	if err != nil {
		// Restore backup on error
		os.Rename(backupPath, dbPath)
		return fmt.Errorf("new key verification failed: %v", err)
	}

	// Step 9: Keep backup file for safety (can be removed manually if needed)
	// os.Remove(backupPath) // Commented out to keep backup for safety

	return nil
}

// InitDB initializes the main database
// If key is provided, the database will be encrypted with that key
// If key is empty string, the database will be unencrypted
func InitDB(key ...string) error {
	var dbKey string
	if len(key) > 0 {
		dbKey = key[0]
	}

	// Initialize connection pool if not already initialized
	InitDBPool(10, 5, 5, 0)

	// For initialization, use write connection
	db, err := GetWriteDB()
	if err != nil {
		// Fallback to old method if pool not available
		db, err = GetDBWithKey(dbKey)
		if err != nil {
			return ERR_OPEN_DB
		}
		defer db.Close()
	}
	// Note: If using pool, don't close the connection

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS bkt (id BIGINT PRIMARY KEY NOT NULL,
		uid BIGINT NOT NULL,
		quota BIGINT NOT NULL,
		used BIGINT NOT NULL,
		real_used BIGINT NOT NULL DEFAULT 0,
		logical_used BIGINT NOT NULL DEFAULT 0,
		dedup_savings BIGINT NOT NULL DEFAULT 0,
		type TINYINT NOT NULL,
		name TEXT NOT NULL,
		chunk_size BIGINT NOT NULL DEFAULT 0,
		key TEXT NOT NULL DEFAULT '',
		ref_level INTEGER NOT NULL DEFAULT 0,
		cmpr_way INTEGER NOT NULL DEFAULT 0,
		wise_cmpr INTEGER NOT NULL DEFAULT 0,
		cmpr_qlty INTEGER NOT NULL DEFAULT 0,
		endec_way INTEGER NOT NULL DEFAULT 0,
		endec_key TEXT NOT NULL DEFAULT ''
	)`)
	if err != nil {
		return fmt.Errorf("%w: create bkt table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS usr (id BIGINT PRIMARY KEY NOT NULL,
		role TINYINT NOT NULL,
		usr TEXT NOT NULL,
		pwd TEXT NOT NULL,
		name TEXT NOT NULL,
		avatar TEXT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create usr table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_uid on bkt (uid)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_uid: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_name on bkt (name)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_name: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_usr on usr (usr)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_usr: %v", ERR_EXEC_DB, err)
	}

	// Create default admin user if no admin exists
	initDefaultAdmin(db)

	return nil
}

// initDefaultAdmin creates a default admin user if no admin user exists
func initDefaultAdmin(db *sql.DB) {
	// Check if any admin user exists
	var count int64
	ctx := context.Background()
	if _, err := b.TableContext(ctx, db, USR_TBL).Select(&count,
		b.Fields("count(1)"),
		b.Where(b.Eq("role", ADMIN))); err != nil {
		return // If query fails, skip creating default admin
	}

	// If no admin exists, create default admin user
	if count == 0 {
		// Default admin credentials: username "orcas", password "orcas"
		hashedPwd, err := HashPassword("orcas")
		if err != nil {
			return // If password hashing fails, skip creating default admin
		}

		// Generate a new ID for the admin user
		adminID := NewID()
		if adminID <= 0 {
			return // If ID generation fails, skip creating default admin
		}

		// Create default admin user
		adminUser := &UserInfo{
			ID:     adminID,
			Role:   ADMIN,
			Usr:    "orcas",
			Pwd:    hashedPwd,
			Name:   "Administrator",
			Avatar: "",
		}
		if _, err = b.TableContext(ctx, db, USR_TBL).Insert(adminUser); err != nil {
			// If insert fails (e.g., user already exists), ignore the error
			return
		}
	}
}

func InitBucketDB(c Ctx, bktID int64) error {
	// Initialize connection pool if not already initialized
	InitDBPool(10, 5, 5, 0)

	// For initialization, use write connection
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		// Fallback to old method if pool not available
		db, err = GetDB(c, bktID)
		if err != nil {
			return fmt.Errorf("%w: %v", ERR_OPEN_DB, err)
		}
		defer db.Close()
	}
	// Note: If using pool, don't close the connection

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS obj (id BIGINT PRIMARY KEY NOT NULL,
		pid BIGINT NOT NULL,
		did BIGINT NOT NULL,
		size BIGINT NOT NULL,
		mtime BIGINT NOT NULL,
		type TINYINT NOT NULL,
		name TEXT NOT NULL,
		ext TEXT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create obj table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE TABLE IF NOT EXISTS data (id BIGINT PRIMARY KEY NOT NULL,
		size BIGINT NOT NULL,
		o_size BIGINT NOT NULL,
		md5 BIGINT NOT NULL,
		pkg_id BIGINT NOT NULL,
		pkg_off INTEGER NOT NULL,
		h_crc32 INTEGER NOT NULL,
		crc32 INTEGER NOT NULL,
		cksum INTEGER NOT NULL,
		kind SMALLINT NOT NULL
	)`)
	if err != nil {
		return fmt.Errorf("%w: create data table: %v", ERR_EXEC_DB, err)
	}

	_, err = db.Exec(`CREATE UNIQUE INDEX IF NOT EXISTS uk_pid_name on obj (pid, name)`)
	if err != nil {
		return fmt.Errorf("%w: create index uk_pid_name: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`CREATE INDEX IF NOT EXISTS ix_ref ON data (o_size, h_crc32, crc32, md5)`)
	if err != nil {
		return fmt.Errorf("%w: create index ix_ref: %v", ERR_EXEC_DB, err)
	}
	_, err = db.Exec(`PRAGMA temp_store = MEMORY`)
	if err != nil {
		return fmt.Errorf("%w: set pragma: %v", ERR_EXEC_DB, err)
	}
	return nil
}

type DefaultMetadataAdapter struct{}

func (dma *DefaultMetadataAdapter) Close() {
}

func (dma *DefaultMetadataAdapter) RefData(c Ctx, bktID int64, d []*DataInfo) ([]int64, error) {
	// Use write connection because we need to create temporary table
	// Temporary tables require write access
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	tbl := fmt.Sprintf("tmp_%x", time.Now().UnixNano())
	// 创建临时表
	db.Exec(`CREATE TEMPORARY TABLE ` + tbl + ` (o_size BIGINT NOT NULL,
		h_crc32 UNSIGNED INT NOT NULL,
		crc32 UNSIGNED INT NOT NULL,
		md5 BIGINT NOT NULL
	)`)
	// 把待查询数据放到临时表
	if _, err = b.TableContext(c, db, tbl).Insert(&d,
		b.Fields("o_size", "h_crc32", "crc32", "md5")); err != nil {
		return nil, ERR_EXEC_DB
	}
	var refs []struct {
		ID       int64  `borm:"max(a.id)"`
		OrigSize int64  `borm:"b.o_size"`
		HdrCRC32 uint32 `borm:"b.h_crc32"`
		CRC32    uint32 `borm:"b.crc32"`
		MD5      int64  `borm:"b.md5"`
	}
	// 联表查询
	if _, err = b.TableContext(c, db, `data a, `+tbl+` b`).Select(&refs,
		b.Join(`on a.o_size=b.o_size and a.h_crc32=b.h_crc32 and 
			(b.crc32=0 or b.md5=0 or (a.crc32=b.crc32 and a.md5=b.md5))`),
		b.GroupBy("b.o_size", "b.h_crc32", "b.crc32", "b.md5")); err != nil {
		return nil, ERR_QUERY_DB
	}
	// 删除临时表
	db.Exec(`DROP TABLE ` + tbl)

	// 构造辅助查询map
	aux := make(map[string]int64, 0)
	for _, ref := range refs {
		aux[fmt.Sprintf("%d:%d:%d:%d", ref.OrigSize, ref.HdrCRC32, ref.CRC32, ref.MD5)] = ref.ID
	}

	res := make([]int64, len(d))
	for i, x := range d {
		// 如果最基础的数据不完整，直接跳过
		if x.OrigSize == 0 || x.HdrCRC32 == 0 {
			continue
		}

		key := fmt.Sprintf("%d:%d:%d:%d", x.OrigSize, x.HdrCRC32, x.CRC32, x.MD5)
		if id, ok := aux[key]; ok {
			// Found in database or current batch
			if id < 0 {
				// Negative ID means reference to another element in current batch (using two's complement)
				// Return the negative index directly (^index format)
				res[i] = id
			} else {
				// Positive ID means found in database
				// 全文件的数据没有，说明是预Ref
				if x.CRC32 == 0 || x.MD5 == 0 {
					if id > 0 {
						res[i] = 1 // 非0代表预Ref成功，预Ref只看数据库
					}
				} else {
					res[i] = id
				}
			}
		} else {
			// 没有秒传成功，但是当前批次可能有一样的数据
			// Store negative index (^i) for future elements in this batch to reference
			aux[key] = int64(^i)
		}
	}
	return res, nil
}

func (dma *DefaultMetadataAdapter) PutData(c Ctx, bktID int64, d []*DataInfo) error {
	// Use write connection for data insertion
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, DATA_TBL).ReplaceInto(&d); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetData(c Ctx, bktID, id int64) (d *DataInfo, err error) {
	// Use read connection for data retrieval
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	d = &DataInfo{}
	if _, err = b.TableContext(c, db, DATA_TBL).Select(d, b.Where(b.Eq("id", id))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListAllData(c Ctx, bktID int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	// Use read connection for listing data
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&total, b.Fields("count(1)")); err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Paginated query: use borm to implement LIMIT and OFFSET
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		if _, err = b.TableContext(c, db, DATA_TBL).Select(&d); err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) FindDuplicateData(c Ctx, bktID int64, offset, limit int) (groups []DuplicateGroup, total int64, err error) {
	// Use read connection for duplicate data search
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 定义结构体来接收GROUP BY的结果
	type duplicateGroupResult struct {
		OrigSize int64  `borm:"o_size"`
		HdrCRC32 uint32 `borm:"h_crc32"`
		CRC32    uint32 `borm:"crc32"`
		MD5      int64  `borm:"md5"`
		IDs      string `borm:"GROUP_CONCAT(id)"`
		Count    int64  `borm:"count(1)"`
		MinID    int64  `borm:"min(id)"`
	}

	// 查询条件
	whereConds := []interface{}{
		b.Gt("o_size", 0),
		b.Gt("h_crc32", 0),
		b.Gt("crc32", 0),
		b.Neq("md5", 0),
	}

	// 先查询所有分组（包括重复的和不重复的）
	var allGroups []duplicateGroupResult
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&allGroups,
		b.Fields("o_size", "h_crc32", "crc32", "md5", "GROUP_CONCAT(id)", "count(1)", "min(id)"),
		b.Where(whereConds...),
		b.GroupBy("o_size", "h_crc32", "crc32", "md5"),
		b.OrderBy("min(id)")); err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// 过滤出重复的组（count > 1）
	var duplicateGroups []duplicateGroupResult
	for _, g := range allGroups {
		if g.Count > 1 {
			duplicateGroups = append(duplicateGroups, g)
		}
	}

	// 计算总数
	total = int64(len(duplicateGroups))
	if total == 0 {
		return groups, 0, nil
	}

	// 分页处理
	start := offset
	end := offset + limit
	if limit <= 0 {
		end = len(duplicateGroups)
	}
	if start > len(duplicateGroups) {
		return groups, total, nil
	}
	if end > len(duplicateGroups) {
		end = len(duplicateGroups)
	}

	// 转换为DuplicateGroup
	for _, g := range duplicateGroups[start:end] {
		// 解析ID列表
		idStrs := strings.Split(g.IDs, ",")
		dataIDs := make([]int64, 0, len(idStrs))
		for _, idStr := range idStrs {
			id, err2 := strconv.ParseInt(strings.TrimSpace(idStr), 10, 64)
			if err2 != nil {
				continue
			}
			dataIDs = append(dataIDs, id)
		}

		if len(dataIDs) > 1 {
			key := fmt.Sprintf("%d:%d:%d:%d", g.OrigSize, g.HdrCRC32, g.CRC32, g.MD5)
			groups = append(groups, DuplicateGroup{
				Key:     key,
				DataIDs: dataIDs,
			})
		}
	}

	return
}

func (dma *DefaultMetadataAdapter) UpdateObjDataID(c Ctx, bktID int64, oldDataID, newDataID int64) error {
	// Use write connection for update operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 更新所有引用oldDataID的对象，将其DataID改为newDataID
	// 使用borm进行批量更新
	updateObj := &ObjectInfo{
		DataID: newDataID,
	}
	if _, err = b.TableContext(c, db, OBJ_TBL).Update(updateObj,
		b.Fields("did"),
		b.Where(b.Eq("did", oldDataID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) DeleteData(c Ctx, bktID int64, dataIDs []int64) error {
	if len(dataIDs) == 0 {
		return nil
	}
	// Use write connection for delete operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// 批量删除数据元信息
	_, err = b.TableContext(c, db, DATA_TBL).Delete(b.Where(b.In("id", dataIDs)))
	if err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) FindSmallPackageData(c Ctx, bktID int64, maxSize int64, offset, limit int) (d []*DataInfo, total int64, err error) {
	// Use read connection for query operation
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Find small file data: pkg_id=0 (not packaged), o_size < maxSize
	// Get total count using borm
	conds := []interface{}{
		b.Eq("pkg_id", 0),
		b.Gt("o_size", 0),
		b.Lt("o_size", maxSize),
	}
	if _, err = b.TableContext(c, db, DATA_TBL).Select(&total,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, ERR_QUERY_DB
	}

	// Paginated query: use borm to implement LIMIT and OFFSET
	if limit > 0 {
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	} else {
		// limit=0 means get all (for backward compatibility, but not recommended)
		_, err = b.TableContext(c, db, DATA_TBL).Select(&d,
			b.Where(conds...),
			b.OrderBy("id"))
		if err != nil {
			return nil, 0, ERR_QUERY_DB
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) PutObj(c Ctx, bktID int64, o []*ObjectInfo) (ids []int64, err error) {
	// Use write connection for object insertion
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	for _, x := range o {
		ids = append(ids, x.ID)
	}

	var n int
	if n, err = b.TableContext(c, db, OBJ_TBL).InsertIgnore(&o); err != nil {
		return nil, ERR_EXEC_DB
	}
	if n != len(o) {
		var inserted []int64
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&inserted, b.Fields("id"), b.Where(b.In("id", ids))); err != nil {
			return nil, ERR_QUERY_DB
		}
		// 处理有冲突的情况
		m := make(map[int64]struct{}, 0)
		for _, v := range inserted {
			m[v] = struct{}{}
		}
		// 擦除没有插入成功的id
		for i, id := range ids {
			if _, ok := m[id]; !ok {
				ids[i] = 0
			}
		}
	}
	return
}

// PutDataAndObj writes both DataInfo and ObjectInfo in a single transaction
// This optimization reduces database round trips by combining two separate writes
func (dma *DefaultMetadataAdapter) PutDataAndObj(c Ctx, bktID int64, d []*DataInfo, o []*ObjectInfo) error {
	// Use write connection for combined write operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Use transaction to ensure atomicity
	tx, err := db.Begin()
	if err != nil {
		return ERR_EXEC_DB
	}
	defer tx.Rollback()

	// Write DataInfo first
	if len(d) > 0 {
		if _, err = b.TableContext(c, tx, DATA_TBL).ReplaceInto(&d); err != nil {
			return ERR_EXEC_DB
		}
	}

	// Write ObjectInfo
	if len(o) > 0 {
		if _, err = b.TableContext(c, tx, OBJ_TBL).ReplaceInto(&o); err != nil {
			return ERR_EXEC_DB
		}
	}

	// Commit transaction
	if err = tx.Commit(); err != nil {
		return ERR_EXEC_DB
	}

	return nil
}

func (dma *DefaultMetadataAdapter) GetObj(c Ctx, bktID int64, ids []int64) (o []*ObjectInfo, err error) {
	// Use read connection for object retrieval
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) SetObj(c Ctx, bktID int64, fields []string, o *ObjectInfo) error {
	// Use write connection for update operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(o, b.Fields(fields...), b.Where(b.Eq("id", o.ID))); err != nil {
		// 如果存在同名文件，会报错：Error: stepping, UNIQUE constraint failed: obj.name (19)
		if strings.Contains(err.Error(), "UNIQUE constraint failed") {
			return ERR_DUP_KEY
		}
		return ERR_EXEC_DB
	}
	return nil
}

func toDelim(field string, o *ObjectInfo) string {
	var d interface{}
	switch field {
	case "id":
		return fmt.Sprint(o.ID)
	case "name":
		return fmt.Sprint(o.Name)
	case "mtime":
		d = o.MTime
	case "size":
		d = o.Size
	case "type":
		d = o.Type
	}
	return fmt.Sprintf("%v:%d", d, o.ID)
}

func doOrder(delim, order string, conds *[]interface{}) (string, string) {
	// 处理order
	if order == "" {
		order = "id"
	}
	fn := b.Gt
	orderBy := order
	switch order[0] {
	case '-':
		fn = b.Lt
		order = order[1:]
		orderBy = order + " desc"
	case '+':
		order = order[1:]
		orderBy = order
	}
	if order != "id" && order != "name" {
		orderBy = orderBy + ", id"
	}

	// 处理边界条件
	ds := strings.Split(delim, ":")
	if len(ds) > 0 && ds[0] != "" {
		if order == "id" || order == "name" {
			*conds = append(*conds, fn(order, ds[0]))
		} else if len(ds) == 2 {
			*conds = append(*conds, b.Or(fn(order, ds[0]),
				b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
		}
	}
	return orderBy, order
}

func (dma *DefaultMetadataAdapter) ListObj(c Ctx, bktID, pid int64,
	wd, delim, order string, count int) (o []*ObjectInfo,
	cnt int64, d string, err error,
) {
	conds := []interface{}{b.Eq("pid", pid)}
	if wd != "" {
		if strings.ContainsAny(wd, "*?") {
			// sqlite 分支使用 LIKE 模式匹配
			conds = append(conds, fmt.Sprintf("name LIKE '%s'", strings.ReplaceAll(strings.ReplaceAll(wd, "*", "%"), "?", "_")))
		} else {
			conds = append(conds, b.Eq("name", wd))
		}
	}

	// Use read connection for listing objects
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, "", ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", ERR_QUERY_DB
	}

	if count > 0 {
		var orderBy string
		orderBy, order = doOrder(delim, order, &conds)
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(count)); err != nil {
			return nil, 0, "", ERR_QUERY_DB
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}

func (dma *DefaultMetadataAdapter) PutUsr(c Ctx, u *UserInfo) error {
	// Use write connection for user creation/update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).ReplaceInto(&u); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetUsr(c Ctx, ids []int64) (o []*UserInfo, err error) {
	// Use read connection for user retrieval
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) GetUsr2(c Ctx, usr string) (o *UserInfo, err error) {
	// Use read connection for user lookup
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	o = &UserInfo{}
	if _, err = b.TableContext(c, db, USR_TBL).Select(o, b.Where(b.Eq("usr", usr))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) SetUsr(c Ctx, fields []string, u *UserInfo) error {
	// Use write connection for user update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Update(&u,
		b.Fields(fields...), b.Where(b.Eq("id", u.ID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) ListUsers(c Ctx) (o []*UserInfo, err error) {
	// Use read connection for listing users
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Use borm to query all users
	if _, err = b.TableContext(c, db, USR_TBL).Select(&o); err != nil {
		return nil, ERR_QUERY_DB
	}

	// 清除敏感信息
	for i := range o {
		o[i].Pwd = ""
	}
	return
}

func (dma *DefaultMetadataAdapter) DeleteUser(c Ctx, userID int64) error {
	// Use write connection for user deletion
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, USR_TBL).Delete(b.Where(b.Eq("id", userID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) PutBkt(c Ctx, o []*BucketInfo) error {
	// Use write connection for bucket creation/update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// CmprWay is now smart compression by default (checks file type)
	for _, bucket := range o {
		if bucket != nil {
			if bucket.ChunkSize <= 0 {
				bucket.ChunkSize = DEFAULT_CHUNK_SIZE
			}
		}
	}

	if _, err = b.TableContext(c, db, BKT_TBL).ReplaceInto(&o); err != nil {
		return ERR_EXEC_DB
	}
	for _, x := range o {
		InitBucketDB(c, x.ID)
	}
	return nil
}

func (dma *DefaultMetadataAdapter) GetBkt(c Ctx, ids []int64) (o []*BucketInfo, err error) {
	// Use read connection for bucket retrieval
	// Pass context to ensure proper database access (for key extraction if needed)
	db, err := GetReadDB(c)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o, b.Where(b.In("id", ids))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListBkt(c Ctx, uid int64) (o []*BucketInfo, err error) {
	// Use read connection for bucket listing
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o, b.Where(b.Eq("uid", uid))); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) ListAllBuckets(c Ctx) (o []*BucketInfo, err error) {
	// Use read connection for listing all buckets
	db, err := GetReadDB()
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Select(&o); err != nil {
		return nil, ERR_QUERY_DB
	}
	return
}

func (dma *DefaultMetadataAdapter) DeleteBkt(c Ctx, bktID int64) error {
	// Use write connection for bucket deletion
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Delete(b.Where(b.Eq("id", bktID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) UpdateBktQuota(c Ctx, bktID int64, quota int64) error {
	// Use write connection for quota update
	db, err := GetWriteDB()
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if _, err = b.TableContext(c, db, BKT_TBL).Update(&BucketInfo{Quota: quota},
		b.Fields("quota"), b.Where(b.Eq("id", bktID))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, size, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktRealUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, -size, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, size, 0, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, -size, 0, 0, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, size, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktLogicalUsed(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, -size, 0)
	return nil
}

func (dma *DefaultMetadataAdapter) IncBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, size)
	return nil
}

func (dma *DefaultMetadataAdapter) DecBktDedupSavings(c Ctx, bktID int64, size int64) error {
	// 使用ecache异步合并刷新，不立即写入数据库
	updateBucketStatsCache(bktID, 0, 0, 0, -size)
	return nil
}

func (dma *DefaultMetadataAdapter) CountDataRefs(c Ctx, bktID int64, dataIDs []int64) (map[int64]int64, error) {
	// Use read connection for counting references
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	if len(dataIDs) == 0 {
		return make(map[int64]int64), nil
	}

	// 统计每个DataID被引用的次数
	type refCount struct {
		DataID int64 `borm:"did"`
		Count  int64 `borm:"count(1)"`
	}
	var counts []refCount
	// 排除已删除的对象（pid >= 0，因为删除时PID会被翻转成负数）
	whereConds := []interface{}{b.In("did", dataIDs), "pid >= 0"}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&counts,
		b.Fields("did", "count(1)"),
		b.Where(whereConds...),
		b.GroupBy("did")); err != nil {
		return nil, ERR_QUERY_DB
	}

	result := make(map[int64]int64)
	for _, c := range counts {
		result[c.DataID] = c.Count
	}
	// 确保所有查询的dataID都在结果中（即使引用数为0）
	for _, dataID := range dataIDs {
		if _, exists := result[dataID]; !exists {
			result[dataID] = 0
		}
	}
	return result, nil
}

func (dma *DefaultMetadataAdapter) ListObjsByType(c Ctx, bktID int64, objType int, offset, limit int) ([]*ObjectInfo, int64, error) {
	// Use read connection for listing objects by type
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	var cnt int64
	// List objects by type, excluding deleted objects (pid >= 0)
	// Use b.And to combine conditions like in other places
	conds := []interface{}{b.Eq("type", objType), b.Gte("pid", 0)}
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...))
	if err != nil {
		// Log detailed error for debugging
		log.Printf("[ListObjsByType] Query failed: bktID=%d, objType=%d, error=%v", bktID, objType, err)
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var objs []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			// Log detailed error for debugging
			log.Printf("[ListObjsByType] Query failed: bktID=%d, objType=%d, offset=%d, limit=%d, error=%v", bktID, objType, offset, limit, err)
			return nil, 0, ERR_QUERY_DB
		}
	}
	return objs, cnt, nil
}

func (dma *DefaultMetadataAdapter) ListChildren(c Ctx, bktID int64, pid int64, offset, limit int) ([]*ObjectInfo, int64, error) {
	// Use read connection for listing children
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, 0, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get total count
	var cnt int64
	// List children of specified parent (pid)
	// Note: We don't need "pid >= 0" here because we're querying children of a specific parent
	// If the parent is deleted (pid < 0), we shouldn't be querying its children anyway
	conds := []interface{}{b.Eq("pid", pid)}
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...))
	if err != nil {
		// Log detailed error for debugging
		log.Printf("[ListChildren] Query failed: bktID=%d, pid=%d, error=%v", bktID, pid, err)
		return nil, 0, ERR_QUERY_DB
	}

	// Get paginated results using borm
	var children []*ObjectInfo
	if limit > 0 {
		_, err = b.TableContext(c, db, OBJ_TBL).Select(&children,
			b.Where(conds...),
			b.OrderBy("id"),
			b.Limit(limit, offset))
		if err != nil {
			// Log detailed error for debugging
			log.Printf("[ListChildren] Query failed: bktID=%d, pid=%d, offset=%d, limit=%d, error=%v", bktID, pid, offset, limit, err)
			return nil, 0, ERR_QUERY_DB
		}
	}
	return children, cnt, nil
}

func (dma *DefaultMetadataAdapter) GetObjByDataID(c Ctx, bktID int64, dataID int64) ([]*ObjectInfo, error) {
	// Use read connection for querying objects by DataID
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	var objs []*ObjectInfo
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
		b.Where(b.Eq("did", dataID)))
	if err != nil {
		return nil, ERR_QUERY_DB
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListVersions(c Ctx, bktID int64, fileID int64, excludeWriting bool) ([]*ObjectInfo, error) {
	// Use read connection for listing versions
	db, err := GetReadDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	var versions []*ObjectInfo
	// Query all version objects (type=3, pid=fileID)
	// If excludeWriting is true, exclude writing versions (name="0")
	conds := []interface{}{b.Eq("pid", fileID), b.Eq("type", OBJ_TYPE_VERSION)}
	if excludeWriting {
		conds = append(conds, b.Neq("name", WritingVersionName))
	}
	// Use "mtime desc" format (lowercase) as borm expects
	_, err = b.TableContext(c, db, OBJ_TBL).Select(&versions,
		b.Where(conds...),
		b.OrderBy("mtime desc"))
	if err != nil {
		// Return ERR_QUERY_DB to maintain compatibility with existing code
		// The detailed error information is logged via DebugLog if needed
		return nil, ERR_QUERY_DB
	}
	return versions, nil
}

func (dma *DefaultMetadataAdapter) DeleteObj(c Ctx, bktID int64, id int64) error {
	// Use write connection for delete operation
	db, err := GetWriteDB(c, bktID)
	if err != nil {
		return ERR_OPEN_DB
	}
	// Note: Don't close the connection, it's from the pool

	// Get object information (read operation, but using same connection for consistency)
	obj := &ObjectInfo{}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(obj, b.Where(b.Eq("id", id))); err != nil {
		return ERR_QUERY_DB
	}

	// Flip object's PID to negative to mark as deleted (so it disappears from original tree structure)
	// Also update MTime to current timestamp
	// Note: If original PID is 0 (ROOT_OID), use -1 as special marker
	newPID := -obj.PID
	if newPID == 0 {
		newPID = -1 // Negative of ROOT_OID is still 0, use -1 as special marker
	}
	currentTime := Now()

	// Check if there's already a deleted object with same name in same parent directory (PID < 0 and |PID| == original PID)
	// If exists, rename current object to avoid conflict
	var conflictingObjs []*ObjectInfo
	conflictCond := []interface{}{b.Eq("pid", newPID), b.Eq("name", obj.Name), b.Neq("id", id)}
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&conflictingObjs,
		b.Where(conflictCond...)); err == nil && len(conflictingObjs) > 0 {
		// Conflict exists, rename current object (add timestamp suffix)
		ext := filepath.Ext(obj.Name)
		nameWithoutExt := strings.TrimSuffix(obj.Name, ext)
		newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, currentTime, ext)
		obj.Name = newName
	}

	// Update object's PID and MTime, also update name if there's a conflict
	updateFields := []string{"pid", "mtime"}
	hasNameChange := len(conflictingObjs) > 0
	if hasNameChange {
		updateFields = append(updateFields, "name")
	}

	if _, err = b.TableContext(c, db, OBJ_TBL).Update(&ObjectInfo{
		ID:    id,
		PID:   newPID,
		MTime: currentTime,
		Name:  obj.Name,
	}, b.Fields(updateFields...), b.Where(b.Eq("id", id))); err != nil {
		return ERR_EXEC_DB
	}
	return nil
}

func (dma *DefaultMetadataAdapter) ListDeletedObjs(c Ctx, bktID int64, beforeTime int64, limit int) ([]*ObjectInfo, error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, ERR_OPEN_DB
	}
	defer db.Close()

	var objs []*ObjectInfo
	// PID < 0 or PID = -1 indicates deleted objects
	conds := []interface{}{"pid < 0 OR pid = -1"}
	if beforeTime > 0 {
		conds = append(conds, b.Lte("mtime", beforeTime))
	}

	if limit > 0 {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime"),
			b.Limit(limit)); err != nil {
			return nil, ERR_QUERY_DB
		}
	} else {
		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&objs,
			b.Where(conds...),
			b.OrderBy("mtime")); err != nil {
			return nil, ERR_QUERY_DB
		}
	}
	return objs, nil
}

func (dma *DefaultMetadataAdapter) ListRecycleBin(c Ctx, bktID int64, opt ListOptions) (o []*ObjectInfo, cnt int64, d string, err error) {
	db, err := GetDB(c, bktID)
	if err != nil {
		return nil, 0, "", ERR_OPEN_DB
	}
	defer db.Close()

	// PID < 0 或 PID = -1 表示已删除的对象
	conds := []interface{}{"pid < 0 OR pid = -1"}

	// Process filter word
	if opt.Word != "" {
		if strings.ContainsAny(opt.Word, "*?") {
			// SQLite branch uses LIKE pattern matching
			conds = append(conds, fmt.Sprintf("name LIKE '%s'", strings.ReplaceAll(strings.ReplaceAll(opt.Word, "*", "%"), "?", "_")))
		} else {
			conds = append(conds, b.Eq("name", opt.Word))
		}
	}

	// Get total count
	if _, err = b.TableContext(c, db, OBJ_TBL).Select(&cnt,
		b.Fields("count(1)"),
		b.Where(conds...)); err != nil {
		return nil, 0, "", ERR_QUERY_DB
	}

	if opt.Count > 0 {
		// Process sorting
		order := opt.Order
		if order == "" {
			order = "mtime"
		}
		fn := b.Gt
		orderBy := order
		switch order[0] {
		case '-':
			fn = b.Lt
			order = order[1:]
			orderBy = order + " desc"
		case '+':
			order = order[1:]
			orderBy = order
		}
		if order != "id" && order != "name" {
			orderBy = orderBy + ", id"
		}

		// Process delimiter (for pagination)
		if opt.Delim != "" {
			ds := strings.Split(opt.Delim, ":")
			if len(ds) > 0 && ds[0] != "" {
				if order == "id" || order == "name" {
					conds = append(conds, fn(order, ds[0]))
				} else if len(ds) == 2 {
					conds = append(conds, b.Or(fn(order, ds[0]),
						b.And(b.Eq(order, ds[0]), b.Gt("id", ds[1]))))
				}
			}
		}

		if _, err = b.TableContext(c, db, OBJ_TBL).Select(&o,
			b.Where(conds...),
			b.OrderBy(orderBy),
			b.Limit(opt.Count)); err != nil {
			return nil, 0, "", ERR_QUERY_DB
		}

		if len(o) > 0 {
			d = toDelim(order, o[len(o)-1])
		}
	}
	return
}
